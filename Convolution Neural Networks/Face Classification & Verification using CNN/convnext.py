# -*- coding: utf-8 -*-
"""Convnext_Paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SwVIcsJ4TwB7Zc9jOMiJGobgEK5hvlhU
"""

# !nvidia-smi # Run this to see what GPU you have

"""#NOTE: RESTART THE RUN TIME AFTER RUNNING THE THE CELL BELOW"""

# Install the necessary packages. Very important: Please restart your session in Colab/Local Machine
# You can restart your session in colab by going to Runtime and then clicking restart session
!pip install wandb --quiet
!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1

# Import the important packages for this Homework. Feel free to add anything here you need.
import torch
from torchsummary import summary
import torchvision # This library is used for image-based operations (Augmentations)
import os
import gc
from tqdm import tqdm
from PIL import Image
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
import glob
import wandb
import matplotlib.pyplot as plt
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device: ", DEVICE)

# from google.colab import drive # Link to your drive if you are not using Colab with GCP
# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here

"""# Kaggle"""

# TODO: Use the same Kaggle code from HW1P2
!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8
!mkdir /root/.kaggle

with open("/root/.kaggle/kaggle.json", "w+") as f:
    f.write('{"username":"vedant100","key":"90f49dc2663e3faa473c8b577bc5393e"}')
    # Put your kaggle username & key here

!chmod 600 /root/.kaggle/kaggle.json

# Reminder: Make sure you have connected your kaggle API before running this block
!mkdir '/content/data'

!kaggle competitions download -c 11785-hw2p2-face-recognition
!unzip -qo '11785-hw2p2-face-recognition.zip' -d '/content/data'

!kaggle competitions download -c 11785-hw2p2-face-verification
!unzip -qo '11785-hw2p2-face-verification.zip' -d '/content/data'

"""# Configs"""

config = {
    'batch_size': 64,  # Standard size; consider increasing if your GPU has ample memory for potentially faster training.
    'lr': 0.1,  # Initial learning rate for SGD; might need fine-tuning based on model performance and dataset.
    'epochs': 100,  # Allowing more epochs for the model to learn, especially useful for deeper architectures.
    'optimizer': 'AdamW',  # Using Stochastic Gradient Descent as the optimizer.
    'momentum': 0.9,  # Momentum helps accelerate SGD in the relevant direction and dampens oscillations.
    'weight_decay': 0.05,  # Regularization term to prevent overfitting, especially important for deeper networks.
    'scheduler': 'CosineAnnealingLR',  # Adjusts the learning rate in a cosine curve manner, beneficial for convergence.
    'T_max': 100,  # The number of steps until the first restart in SGDR (Stochastic Gradient Descent with Warm Restarts).
    'eta_min': 1e-6,  # The minimum learning rate, ensuring the learning rate doesn't decrease to 0 during training.
    'grad_clip': 5.0,  # Clipping gradients to a maximum value to prevent exploding gradients in deep networks.
    'use_amp': True,  # Enable automatic mixed precision for faster training and reduced memory use on compatible GPUs.
    # Feel free to add other parameters as needed for your specific setup and goals.
}

"""# Dataset

## Datasets and Dataloaders
"""

# --------------------------------------------------- #

# Data paths

DATA_DIR    = "/content/data/11-785-s24-hw2p2-classification" # TODO: Path where you have downloaded the classificaation data
TRAIN_DIR   = os.path.join(DATA_DIR, "train")
VAL_DIR     = os.path.join(DATA_DIR, "dev")
TEST_DIR    = os.path.join(DATA_DIR, "test")

# --------------------------------------------------- EDIT THIS#
# train_transforms = torchvision.transforms.Compose([
#     torchvision.transforms.RandomResizedCrop(224),  # Randomly crops the image and resizes it to 224x224
#     torchvision.transforms.RandomHorizontalFlip(),  # Randomly flips the image horizontally
#     torchvision.transforms.ToTensor(),              # Converts the image to a tensor
#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizes the tensor with mean and std
# ])




# Transformations/augmentations of the Train dataset

"""
Refer https://pytorch.org/vision/stable/transforms.html
Implementing the right train transforms/augmentation methods is key to improving performance.
Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()
But there are some transforms which are performed after ToTensor() : e.g - Normalization
#Normalization Tip - Do not blindly use normalization that is not suitable for this dataset
"""
import torchvision.transforms as transforms
# class ClipTransform:
#     def _init_(self, min_value=0, max_value=1):
#         self.min_value = min_value
#         self.max_value = max_value

#     def _call_(self, tensor):
#         # Clip the tensor values
#         tensor = torch.clamp(tensor, self.min_value, self.max_value)
#         return tensor

train_transforms = transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(p=0.4),
    torchvision.transforms.RandomPerspective(distortion_scale=0.25, p=0.4),
    torchvision.transforms.GaussianBlur(kernel_size=3),
    torchvision.transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),
    # torchvision.transforms.Normalize(mean=mean,Â std=std),
    transforms.Normalize(mean=[0.5103, 0.4014, 0.3509], std=[0.2708, 0.2363, 0.2226]),
    transforms.RandomErasing(p=0.25, scale=(0.02, 0.33))  # Random erasing with a 50% probability
]) # TODO: Specify transformations/augmentations performed on the train dataset

# --------------------------------------------------- #


# Transformations/augmentations of the Val dataset

"""
You should NOT have data augmentation on the validation set. Why?
"""

valid_transforms = torchvision.transforms.Compose([
    #torchvision.transforms.Resize(256),
    #torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    #ClipTransform(min_value=0, max_value=1),  # Ensure pixel values are within [0, 1]
    torchvision.transforms.Normalize(mean=[0.5103, 0.4014, 0.3509], std=[0.2708, 0.2363, 0.2226])
]) # TODO: Specify transformations performed on the val dataset

# --------------------------------------------------- #

# Initializing the train and val datasets

train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)
valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform = valid_transforms)

# --------------------------------------------------- #

# Initializing the train and val dataloaders

train_loader = torch.utils.data.DataLoader(dataset       = train_dataset,
                                           batch_size    = config['batch_size'],
                                           shuffle        = True,
                                           num_workers = 4, # Uncomment this line if you want to increase your num workers
                                           pin_memory    = True)

valid_loader = torch.utils.data.DataLoader(dataset       = valid_dataset,
                                           batch_size    = config['batch_size'],
                                           shuffle        = False,
                                           num_workers = 2 # Uncomment this line if you want to increase your num workers
                                           )

# --------------------------------------------------- #

# Test dataset class

"""
You can do this with ImageFolder as well, but it requires some tweaking
"""

class TestDataset(torch.utils.data.Dataset):

    def __init__(self, data_dir, transforms):
        self.data_dir   = data_dir
        self.transforms = transforms

        # This one-liner basically generates a sorted list of full paths to each image in the test directory
        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))

    def _len_(self):
        return len(self.img_paths)

    def _getitem_(self, idx):
        return self.transforms(Image.open(self.img_paths[idx]))

# --------------------------------------------------- #

# Initializing the test dataset

"""
Why are we using val_transforms for Test Data?
"""

test_dataset = TestDataset(TEST_DIR, transforms = valid_transforms)

# --------------------------------------------------- #

# Initializing the test dataloader

test_loader = torch.utils.data.DataLoader(dataset    = test_dataset,
                                          batch_size = config['batch_size'],
                                          shuffle     = False,
                                          drop_last  = False,
                                          num_workers = 2 # Uncomment this line if you want to increase your num workers
                                          )

"""## EDA and Viz"""

# Double-check your dataset/dataloaders work as expected

print("Number of classes    : ", len(train_dataset.classes))
print("No. of train images  : ", train_dataset.__len__())
print("Shape of image       : ", train_dataset[0][0].shape)
print("Batch size           : ", config['batch_size'])
print("Train batches        : ", train_loader.__len__())
print("Val batches          : ", valid_loader.__len__())

# Feel free to print more things if needed

# Visualize a few images in the dataset

"""
You can write your own code, and you don't need to understand the code
It is highly recommended that you visualize your data augmentation as sanity check
"""

r, c    = [5, 5]
fig, ax = plt.subplots(r, c, figsize= (15, 15))

k       = 0
dtl     = torch.utils.data.DataLoader(
    dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms
    batch_size  = config['batch_size'],
    shuffle     = True)

for data in dtl:
    x, y = data

    for i in range(r):
        for j in range(c):
            img = x[k].numpy().transpose(1, 2, 0)
            ax[i, j].imshow(img)
            ax[i, j].axis('off')
            k+=1
    break

del dtl

!pip install timm

import torch
import torch.nn as nn
from torchsummary import summary
import torch.nn.functional as F
from timm.models.layers import trunc_normal_, DropPath
# Sphereface loss no of blocks is 2 2 6 2 number of channels is 80 161 322 644 weight decay is 1e-4

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x

class InvertedBottleneckBlock(nn.Module):
    def __init__(self, in_channels, out_channels, expansion_factor, drop_path = 0):
        super().__init__()
        layer_scale_init_value = 1e-6
        # self.survival_prob = survival_prob
        self.use_residual = in_channels == out_channels
        expanded_channels = in_channels * expansion_factor

        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=7, padding = 3, groups = in_channels)
        self.pointwise1 = nn.Linear(in_channels, expanded_channels)
        self.pointwise2 = nn.Linear(expanded_channels, in_channels)
        self.ln1 = LayerNorm(in_channels, eps = 1e-6)
        self.gelu = nn.GELU()

        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(in_channels),
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
    def forward(self, x):
        # print(f"Input to InvertedBottleneckBlock: {x.shape}")
        residual = x
        x = self.depthwise(x)
        x = x.permute(0, 2, 3, 1)
        x = self.ln1(x)
        x = self.pointwise1(x)
        x = self.gelu(x)
        x = self.pointwise2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2)
        x = residual + self.drop_path(x)
        # print(f"Output from InvertedBottleneckBlock: {x.shape}")
        return x
class Network(nn.Module):
    def __init__(self, num_classes=7001):
        super(Network, self).__init__()
        initial_conv = nn.Conv2d(3, 80, kernel_size=4, stride=4)
        initial_ln = LayerNorm(80, eps=1e-6, data_format="channels_first")

        stage1_ln = LayerNorm(80, eps=1e-6, data_format="channels_first")
        stage1_downsample = nn.Conv2d(80, 80, kernel_size=2, stride=2)
        stage1_block1 = InvertedBottleneckBlock(80, 80, expansion_factor=4)
        stage1_block2 = InvertedBottleneckBlock(80, 80, expansion_factor=4)
        # stage1_block3 = InvertedBottleneckBlock(80, 80, expansion_factor=4)

        stage2_ln = LayerNorm(80, eps = 1e-6, data_format = "channels_first")
        stage2_downsample = nn.Conv2d(80, 161, kernel_size=2, stride=2)
        stage2_block1 = InvertedBottleneckBlock(161, 161, expansion_factor=4)
        stage2_block2 = InvertedBottleneckBlock(161, 161, expansion_factor=4)
        # stage2_block3 = InvertedBottleneckBlock(161, 161, expansion_factor=4)

        stage3_ln = LayerNorm(161, eps = 1e-6, data_format = "channels_first")
        stage3_downsample = nn.Conv2d(161, 322, kernel_size=2, stride=2)
        stage3_block1 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        stage3_block2 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        stage3_block3 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        stage3_block4 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        stage3_block5 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        stage3_block6 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        # stage3_block7 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        # stage3_block8 = InvertedBottleneckBlock(322, 322, expansion_factor=4)
        # stage3_block9 = InvertedBottleneckBlock(322, 322, expansion_factor=4)

        stage4_ln = LayerNorm(322, eps = 1e-6, data_format = "channels_first")
        stage4_downsample = nn.Conv2d(322, 644, kernel_size=2, stride=2, padding = 1)
        stage4_block1 = InvertedBottleneckBlock(644, 644, expansion_factor=4)
        stage4_block2 = InvertedBottleneckBlock(644, 644, expansion_factor=4)
        # stage4_block3 = InvertedBottleneckBlock(644, 644, expansion_factor=4)

        final_norm = LayerNorm(644, eps=1e-6, data_format = "channels_first")
        # classifier = nn.Linear(644, num_classes)

        # Use layers in nn.Sequential without assignments
        self.backbone = nn.Sequential(
            initial_conv,
            initial_ln,
            stage1_ln,
            stage1_downsample,
            stage1_block1,
            stage1_block2,
            # stage1_block3,
            stage2_ln,
            stage2_downsample,
            stage2_block1,
            stage2_block2,
            # stage2_block3,
            stage3_ln,
            stage3_downsample,
            stage3_block1,
            stage3_block2,
            stage3_block3,
            stage3_block4,
            stage3_block5,
            stage3_block6,
            # stage3_block7,
            # stage3_block8,
            # stage3_block9,
            stage4_ln,
            stage4_downsample,
            stage4_block1,
            stage4_block2,
            # stage4_block3,
            final_norm,
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            # classifier
        )
        self.cls_layer = nn.Sequential(
            nn.Linear(644, 644),
            nn.GELU(),
            # nn.Dropout(0.15),
            nn.Linear(644, num_classes)
        )
    def forward(self, x, return_feats=False):
        # print(f"Input to Network: {x.shape}")
        feats = self.backbone(x)
        # print(f"Output from backbone: {feats.shape}")
        if return_feats:
            return feats
        x = self.cls_layer(feats)
        # print(f"Output from Network: {x.shape}")
        return x


# # Device configuration
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Model initialization
model = Network().to(DEVICE)

summary(model, input_size=(3, 224, 224))
# checkpoint = torch.load('checkpoint_classification.pth')
# model.load_state_dict(checkpoint['model_state_dict'])

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from timm.models.layers import trunc_normal_, DropPath

# class LayerNorm(nn.Module):
#     def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.eps = eps
#         self.data_format = data_format
#         if self.data_format not in ["channels_last", "channels_first"]:
#             raise NotImplementedError
#         self.normalized_shape = (normalized_shape,)

#     def forward(self, x):
#         if self.data_format == "channels_last":
#             return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
#         elif self.data_format == "channels_first":
#             u = x.mean(1, keepdim=True)
#             s = (x - u).pow(2).mean(1, keepdim=True)
#             x = (x - u) / torch.sqrt(s + self.eps)
#             x = self.weight[:, None, None] * x + self.bias[:, None, None]
#             return x

# class Block(nn.Module):
#     def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
#         super().__init__()
#         self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
#         self.norm = LayerNorm(dim, eps=1e-6)
#         self.pwconv1 = nn.Linear(dim, 4 * dim)
#         self.act = nn.GELU()
#         self.pwconv2 = nn.Linear(4 * dim, dim)
#         self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value > 0 else None
#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

#     def forward(self, x):
#         input = x
#         x = self.dwconv(x)
#         x = x.permute(0, 2, 3, 1)
#         x = self.norm(x)
#         x = self.pwconv1(x)
#         x = self.act(x)
#         x = self.pwconv2(x)
#         if self.gamma is not None:
#             x = self.gamma * x
#         x = x.permute(0, 3, 1, 2)
#         x = input + self.drop_path(x)
#         return x

# class ConvNeXt(nn.Module):
#     def __init__(self, in_chans=3, num_classes=7001, depths=[2, 2, 6, 2], dims=[80, 161, 322, 644], drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1.):
#         super().__init__()

#         self.downsample_layers = nn.ModuleList()
#         stem = nn.Sequential(
#             nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
#             LayerNorm(dims[0], eps=1e-6, data_format="channels_first")
#         )
#         self.downsample_layers.append(stem)
#         for i in range(3):
#             downsample_layer = nn.Sequential(
#                 LayerNorm(dims[i], eps=1e-6, data_format="channels_first"),
#                 nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),
#             )
#             self.downsample_layers.append(downsample_layer)

#         self.stages = nn.ModuleList()
#         dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
#         for i in range(4):
#             stage = nn.Sequential(
#                 *[Block(dim=dims[i], drop_path=dp_rates[sum(depths[:i]):sum(depths[:i+1])][j], layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]
#             )
#             self.stages.append(stage)

#         self.norm = nn.LayerNorm(dims[-1], eps=1e-6)
#         self.cls_layer = nn.Sequential(
#             nn.Linear(dims[-1], dims[-1]),
#             nn.GELU(),
#             nn.Linear(dims[-1], num_classes)
#         )

#         self.apply(self._init_weights)
#         # Apply head_init_scale to the final layer in cls_layer
#         last_linear_layer = self.cls_layer[-1]
#         trunc_normal_(last_linear_layer.weight, std=.02)
#         nn.init.constant_(last_linear_layer.bias, 0)
#         last_linear_layer.weight.data.mul_(head_init_scale)
#         last_linear_layer.bias.data.mul_(head_init_scale)

#     def _init_weights(self, m):
#         if isinstance(m, (nn.Conv2d, nn.Linear)):
#             trunc_normal_(m.weight, std=.02)
#             if hasattr(m, 'bias') and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)

#     def forward_features(self, x):
#         for i in range(4):
#             x = self.downsample_layers[i](x)
#             x = self.stages[i](x)
#         return self.norm(x.mean([-2, -1]))

#     def forward(self, x, return_feats=False):
#         feats = self.forward_features(x)
#         if return_feats:
#             return feats
#         x = self.cls_layer(feats)
#         return x


# def convnext_tiny(pretrained=False,in_22k=False, **kwargs):
#     model = ConvNeXt(depths=[2, 2, 6, 2], dims=[80, 161, 322, 644], **kwargs)
#     return model
# from torchsummary import summary

# # Assuming you have already defined the ConvNeXt model as shown in your code snippet
# model = convnext_tiny()  # Initialize your model

# # Set the model to evaluation mode and move it to the appropriate device
# model.eval()
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)

# # Print the summary
# # The input size (3, 224, 224) corresponds to the standard input size for ConvNets (channels, height, width)
# summary(model, input_size=(3, 224, 224))

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from timm.models.layers import trunc_normal_, DropPath

# class LayerNorm(nn.Module):
#     def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.eps = eps
#         self.data_format = data_format
#         if self.data_format not in ["channels_last", "channels_first"]:
#             raise NotImplementedError
#         self.normalized_shape = (normalized_shape,)

#     def forward(self, x):
#         if self.data_format == "channels_last":
#             return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
#         elif self.data_format == "channels_first":
#             u = x.mean(1, keepdim=True)
#             s = (x - u).pow(2).mean(1, keepdim=True)
#             x = (x - u) / torch.sqrt(s + self.eps)
#             x = self.weight[:, None, None] * x + self.bias[:, None, None]
#             return x

# class Block(nn.Module):
#     def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
#         super().__init__()
#         self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
#         self.norm = LayerNorm(dim, eps=1e-6)
#         self.pwconv1 = nn.Linear(dim, 4 * dim)
#         self.act = nn.GELU()
#         self.pwconv2 = nn.Linear(4 * dim, dim)
#         self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value > 0 else None
#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

#     def forward(self, x):
#         input = x
#         x = self.dwconv(x)
#         x = x.permute(0, 2, 3, 1)
#         x = self.norm(x)
#         x = self.pwconv1(x)
#         x = self.act(x)
#         x = self.pwconv2(x)
#         if self.gamma is not None:
#             x = self.gamma * x
#         x = x.permute(0, 3, 1, 2)
#         x = input + self.drop_path(x)
#         return x

# class ConvNeXt(nn.Module):
#     def __init__(self, in_chans=3, num_classes=7001, depths=[2, 2, 6, 2], dims=[80, 161, 322, 644], drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1.):
#         super().__init__()

#         self.downsample_layers = nn.ModuleList()
#         stem = nn.Sequential(
#             nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
#             LayerNorm(dims[0], eps=1e-6, data_format="channels_first")
#         )
#         self.downsample_layers.append(stem)
#         for i in range(3):
#             downsample_layer = nn.Sequential(
#                 LayerNorm(dims[i], eps=1e-6, data_format="channels_first"),
#                 nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),
#             )
#             self.downsample_layers.append(downsample_layer)

#         self.stages = nn.ModuleList()
#         dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
#         for i in range(4):
#             stage = nn.Sequential(
#                 *[Block(dim=dims[i], drop_path=dp_rates[sum(depths[:i]):sum(depths[:i+1])][j], layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]
#             )
#             self.stages.append(stage)

#         self.norm = nn.LayerNorm(dims[-1], eps=1e-6)
#         self.cls_layer = nn.Sequential(
#             nn.Linear(dims[-1], dims[-1]),
#             nn.GELU(),
#             nn.Linear(dims[-1], num_classes)
#         )

#         self.apply(self._init_weights)
#         # Apply head_init_scale to the final layer in cls_layer
#         last_linear_layer = self.cls_layer[-1]
#         trunc_normal_(last_linear_layer.weight, std=.02)
#         nn.init.constant_(last_linear_layer.bias, 0)
#         last_linear_layer.weight.data.mul_(head_init_scale)
#         last_linear_layer.bias.data.mul_(head_init_scale)

#     def _init_weights(self, m):
#         if isinstance(m, (nn.Conv2d, nn.Linear)):
#             trunc_normal_(m.weight, std=.02)
#             if hasattr(m, 'bias') and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)

#     def forward_features(self, x):
#         for i in range(4):
#             x = self.downsample_layers[i](x)
#             x = self.stages[i](x)
#         return self.norm(x.mean([-2, -1]))

#     def forward(self, x, return_feats=False):
#         feats = self.forward_features(x)
#         if return_feats:
#             return feats
#         x = self.cls_layer(feats)
#         return x


# # # Assuming you have already defined the ConvNeXt model as shown in your code snippet
# # model = convnext_tiny()  # Initialize your model

# # # Set the model to evaluation mode and move it to the appropriate device
# # model.eval()
# # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# # model.to(device)

# # # Print the summary
# # # The input size (3, 224, 224) corresponds to the standard input size for ConvNets (channels, height, width)
# # summary(model, input_size=(3, 224, 224))


# model = ConvNeXt()
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model.to(device)

# # Print the summary
# # The input size (3, 224, 224) corresponds to the standard input size for ConvNets (channels, height, width)
# summary(model, input_size=(3, 224, 224))

# Defining Loss function
criterion = torch.nn.CrossEntropyLoss() # TODO: What loss do you need for a multi class classification problem and would label smoothing be beneficial here?


optimizer = torch.optim.AdamW(model.parameters(),
                            lr=0.001,
                            # momentum=0.9,
                            weight_decay=1e-3) # TODO: Feel free to pick a different optimizer
 # TODO: Feel free to pick a different optimizer

# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
# epoch = checkpoint['epoch']
# val_acc = checkpoint['val_acc']
#loss = checkpoint['loss']

# --------------------------------------------------- #

# Adding a scheduler with warmup
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR
# base_lr = config['lr']
# total_epochs = config['epochs']
# warmup_epochs = 20  # Number of epochs for the warm-up
# min_lr = config['eta_min']

# # Main Scheduler - Cosine Annealing after warm-up
# cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs, eta_min=min_lr)

# # Warm-up Scheduler - Linear increase over warmup_epochs
# warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)

# Chaining Schedulers - Warmup followed by Cosine Annealing
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.75, patience=1)
# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
# --------------------------------------------------- #

# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you
# It is useful only in the case of compatible GPUs such as T4/V100
scaler = torch.cuda.amp.GradScaler()

"""# Training and Validation Functions

## Classification Task
"""

def train(model, dataloader, optimizer, criterion, scheduler):
    model.train()
    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)

    num_correct = 0
    total_loss = 0

    for i, (images, labels) in enumerate(dataloader):
        optimizer.zero_grad()
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)

        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())
        total_loss += float(loss.item())

        batch_bar.set_postfix(
            acc="{:.04f}%".format(100 * num_correct / (config['batch_size'] * (i + 1))),
            loss="{:.04f}".format(float(total_loss / (i + 1))),
            num_correct=num_correct,
            lr="{:.04f}".format(float(optimizer.param_groups[0]['lr']))
        )

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        batch_bar.update()
    # scheduler.step()
    batch_bar.close()


    acc = 100 * num_correct / (config['batch_size'] * len(dataloader))
    total_loss = float(total_loss / len(dataloader))

    return acc, total_loss

def validate(model, dataloader, criterion):

    model.eval()
    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)

    num_correct = 0.0
    total_loss = 0.0

    for i, (images, labels) in enumerate(dataloader):

        # Move images to device
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        # Get model outputs
        with torch.inference_mode():
            outputs = model(images)
            loss = criterion(outputs, labels)

        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())
        total_loss += float(loss.item())

        batch_bar.set_postfix(
            acc="{:.04f}%".format(100 * num_correct / (config['batch_size']*(i + 1))),
            loss="{:.04f}".format(float(total_loss / (i + 1))),
            num_correct=num_correct)

        batch_bar.update()

    batch_bar.close()
    acc = 100 * num_correct / (config['batch_size']* len(dataloader))
    total_loss = float(total_loss / len(dataloader))
    return acc, total_loss

gc.collect() # These commands help you when you face CUDA OOM error
torch.cuda.empty_cache()

"""## Verification Task

The verification task consists of the following generalized scenario:
- You are given X unknown identitites
- You are given Y known identitites
- Your goal is to match X unknown identities to Y known identities.

We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.

Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.

This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification.
"""

# This obtains the list of known identities from the known folder
known_regex = "/content/data/11-785-s24-hw2p2-verification/known/*/*"
known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]

# Obtain a list of images from unknown folders
unknown_dev_regex = "/content/data/11-785-s24-hw2p2-verification/unknown_dev/*"
unknown_test_regex = "/content/data/11-785-s24-hw2p2-verification/unknown_test/*"

# We load the images from known and unknown folders
unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]
unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]
known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]

# Why do you need only ToTensor() here?
transforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()])

unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])
unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])
known_images  = torch.stack([transforms(y) for y in known_images ])
#Print your shapes here to understand what we have done

# You can use other similarity metrics like Euclidean Distance if you wish
similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)

import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
import pandas as pd
from sklearn.metrics import accuracy_score

def cosine_similarity(a, b):
    return F.cosine_similarity(a.unsqueeze(1), b.unsqueeze(0), dim=2)

def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):

    unknown_feats, known_feats = [], []

    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)
    model.eval()

    # We load the images as batches for memory optimization and avoiding CUDA OOM errors
    for i in range(0, unknown_images.shape[0], batch_size):
        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size

        with torch.no_grad():
            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model
        unknown_feats.append(unknown_feat)
        batch_bar.update()

    batch_bar.close()

    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)

    for i in range(0, known_images.shape[0], batch_size):
        known_batch = known_images[i:i+batch_size]
        with torch.no_grad():
              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)

        known_feats.append(known_feat)
        batch_bar.update()

    batch_bar.close()

    # Concatenate all the batches
    unknown_feats = torch.cat(unknown_feats, dim=0)
    known_feats = torch.cat(known_feats, dim=0)

    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])
    # Print the inner list comprehension in a separate cell - what is really happening?

    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?
    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()


    # Note that in unknown identities, there are identities without correspondence in known identities.
    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain
    # threshold compared with those identities with correspondence.

    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value
    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings

    # After early submission, remove the previous line and uncomment the following code

    threshold = 0.5
    NO_CORRESPONDENCE_LABEL = 'n000000'
    pred_id_strings = []
    for idx, prediction in enumerate(predictions):
        if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric
            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)
        else:
            pred_id_strings.append(known_paths[prediction])

    if mode == 'val':
      true_ids = pd.read_csv('/content/data/11-785-s24-hw2p2-verification/verification_dev.csv')['label'].tolist()
      accuracy = 100 * accuracy_score(pred_id_strings, true_ids)
      #print("Verification Accuracy = {}".format(accuracy))
      return accuracy, pred_id_strings

    elif mode == 'test':
      return pred_id_strings

gc.collect() # These commands help you when you face CUDA OOM error
torch.cuda.empty_cache()

"""# Wandb"""

wandb.login(key="4c86d0f2bd3d0f4cd0c81ea3862c4119ec70e8e4") # API Key is in your wandb account, under settings (wandb.ai/settings)

# Create your wandb run
run = wandb.init(
    name = "ConvNeXt- VS- Kill_meCV", ## Wandb creates random run names if you skip this field
    reinit = True, ### Allows reinitalizing runs when you re-run this cell
    # run_id = ### Insert specific run id here if you want to resume a previous run
    # resume = "must" ### You need this to resume previous runs, but comment out reinit = True when using this
    project = "hw2p2-ablations", ### Project should be created in your wandb account
    config = config ### Wandb Config for your run
)

"""# Experiments"""

import torch
from tqdm.auto import tqdm

# Assuming DEVICE, model, train_loader, valid_loader, optimizer, criterion, config are defined

best_class_acc      = 0.0
best_ver_acc        = 0.0

for epoch in range(config['epochs']):

    print("\nEpoch {}/{}".format(epoch+1, config['epochs']))

    curr_lr = float(optimizer.param_groups[0]['lr'])

    train_acc, train_loss = train(model, train_loader, optimizer, criterion, scheduler)

    print("\nEpoch {}/{}: \nTrain Acc (Classification) {:.04f}%\t Train Loss (Classification) {:.04f}\t Learning Rate {:.04f}".format(
        epoch + 1, config['epochs'], train_acc, train_loss, curr_lr))

    val_acc, val_loss = validate(model, valid_loader, criterion)
    print("Val Acc (Classification) {:.04f}%\t Val Loss (Classification) {:.04f}".format(val_acc, val_loss))

    scheduler.step(val_loss)

    ver_acc, pred_id_strings = eval_verification(unknown_dev_images, known_images,
                                                 model, similarity_metric, config['batch_size'], mode='val')

    print("Val Acc (Verification) {:.04f}%\t ".format(ver_acc))

    wandb.log({"train_classification_acc": train_acc,
               "train_classification_loss":train_loss,
               "val_classification_acc": val_acc,
               "val_classification_loss": val_loss,
               "val_verification_acc": ver_acc,
               "learning_rate": curr_lr})

    # If you are using a scheduler in your train function within your iteration loop,
    # How will you step your scheduler ?

    if val_acc >= best_class_acc:
        best_valid_acc = val_acc
        torch.save({'model_state_dict':model.state_dict(),
                    'optimizer_state_dict':optimizer.state_dict(),
                    'scheduler_state_dict':scheduler.state_dict(),
                    'val_acc': val_acc,
                    'epoch': epoch}, './checkpoint_classification.pth')
        wandb.save('checkpoint_verification.pth')
        print("Saved best classification model")

    if ver_acc >= best_ver_acc:
      best_ver_acc = ver_acc
      torch.save({'model_state_dict':model.state_dict(),
                  'optimizer_state_dict':optimizer.state_dict(),
                  'scheduler_state_dict':scheduler.state_dict(),
                  'val_acc': ver_acc,
                  'epoch': epoch}, './checkpoint_verification.pth')
      wandb.save('checkpoint_verification.pth')
      print("Saved verification model")

### Finish your wandb run
run.finish()

"""# Testing and Kaggle Submission"""

def test(model,dataloader): # TODO: Run to finish predicting on the test set.

  model.eval()
  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')
  test_results = []

  for i, (images) in enumerate(dataloader):

      images = images.to(DEVICE)

      with torch.inference_mode():
        outputs = model(images)

      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()
      test_results.extend(outputs)

      batch_bar.update()

  batch_bar.close()
  return test_results

test_results = test(model, test_loader)

pred_id_strings = eval_verification(unknown_test_images, known_images,
                                                 model, similarity_metric, config['batch_size'], mode='test')

# TODO: Run to write the results in the files and submit to Kaggle
# You only have 10 Kaggle submissions per day

with open("classification_early_submission.csv", "w+") as f:
    f.write("id,label\n")
    for i in range(len(test_dataset)):
        f.write("{},{}\n".format(str(i).zfill(6) + ".jpg", test_results[i]))

with open("verification_early_submission.csv", "w+") as f:
    f.write("id,label\n")
    for i in range(len(pred_id_strings)):
        f.write("{},{}\n".format(i, pred_id_strings[i]))

# kaggle competitions submit -c 11785-hw2p2-face-recognition -f submission.csv -m "Early-Submission"
# kaggle competitions submit -c 11785-hw2p2-face-verification -f submission.csv -m "Early-Submission"

"""# Finetune your Model

You can choose any model for finetuning. It is a good practice to try to sweep through all the models to find what is the best finetuning model for you!

## Model Definition (finetuning)
"""

# add your finetune/retrain code here

"""## Loss, Optimizer, and Scheduler Definition (finetuning)"""

# add your finetune/retrain code here

"""## Training Function (finetuning)"""

# add your finetune/retrain code here

"""## Wandb (finetuning)"""

# add your finetune/retrain code here

"""## Experiments (finetuning)"""

# add your finetune/retrainÂ codeÂ here