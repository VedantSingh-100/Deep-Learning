{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 27 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwYu9sSUnSho"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummaryX wandb --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sB6afpc4qwZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI4qfx7tiBZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7352552d-0835-4f63-a006-edf6bfa55e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "from torch.optim.lr_scheduler import StepLR  # Add this import statement\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yBgXjKV1O0Z"
      },
      "outputs": [],
      "source": [
        "# ### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPBUd7Cnl-Rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903b1b21-8aeb-41e1-f6f5-4f23396df198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"arushijain99\",\"key\":\"d8d74b4e9db19caa2853582b50318479\"}')\n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if2Somqfbje1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab5a0a0-5b5a-4756-90e0-66aa0591b3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11785-hw1p2-s24.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# commands to download data from kaggle\n",
        "!kaggle competitions download -c 11785-hw1p2-s24\n",
        "\n",
        "!unzip -qo /content/11785-hw1p2-s24.zip -d '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpLCvi3AJC5z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Dataset class to load train and validation data\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = os.path.join(root, partition, \"mfcc\")\n",
        "        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
        "\n",
        "        self.transcript_dir = os.path.join(root,partition,\"transcript\")\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "        # TODO: List files in self.transcript_dir using os.listdir in sorted order\n",
        "        transcript_names    = sorted(os.listdir(self.transcript_dir))\n",
        "        # if len(mfcc_names) > 20000:\n",
        "        #   mfcc_names = mfcc_names[:19000]\n",
        "        #   transcript_names = transcript_names[:19000]\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        =  np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            mean = np.mean(mfcc, axis=0)\n",
        "            std = np.std(mfcc, axis=0)\n",
        "            mfcc = (mfcc - mean) / (std)\n",
        "        #   Load the corresponding transcript\n",
        "\n",
        "            transcript=np.load(os.path.join(self.transcript_dir,transcript_names[i]))[1:-1]\n",
        "\n",
        "            #with open(transcript_path, 'r') as file:\n",
        "              #transcript = file.read().strip().split()\n",
        "            #transcript  = NotImplemented # Remove [SOS] and [EOS] from the transcript\n",
        "            #transcript = transcript[1:-1]\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2), (T2+2) before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that\n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
        "        self.mfccs          = np.concatenate(self.mfccs, axis=0)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that\n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts    = np.concatenate(self.transcripts)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Take some time to think about what we have done.\n",
        "        # self.mfcc is an array of the format (Frames x Features).\n",
        "        # Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is.\n",
        "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "        self.mfccs = np.pad(self.mfccs, ((context, context), (0, 0)), mode='constant', constant_values=0) # TODO\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "        unique_phonemes = {phoneme:i for i,phoneme in enumerate(PHONEMES)}\n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        self.transcripts = np.array([unique_phonemes[phoneme] for phoneme in self.transcripts])\n",
        "\n",
        "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind: ind + 2 * self.context + 1, :]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten() # TODO: Flatten to get 1d data\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, phonemes=PHONEMES, context=0, partition=\"test-clean\"):  # Fix: Double underscores\n",
        "        self.context = context\n",
        "        self.mfcc_dir = os.path.join(root, 'mfcc')\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.mfccs = []\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n",
        "            mean = np.mean(mfcc, axis= 0)\n",
        "            std = np.std(mfcc, axis=0)\n",
        "            mfcc = (mfcc - mean) / std\n",
        "            self.mfccs.append(mfcc)\n",
        "        self.mfccs = np.concatenate(self.mfccs)\n",
        "        self.length = len(self.mfccs)\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        start = ind\n",
        "        end = ind + (2 * self.context) + 1\n",
        "        frames = self.mfccs[start:end, :]\n",
        "        frames = np.ndarray.flatten(frames)\n",
        "        frames = torch.FloatTensor(frames)\n",
        "        return frames\n"
      ],
      "metadata": {
        "id": "3oV4tuom1E_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNacQ8bpt9nw"
      },
      "source": [
        "# Parameters Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmKwlFqgt_Zq"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs'        : 55,\n",
        "    #'batch_size'    : 1024,\n",
        "    'batch_size'    : 2048,\n",
        "    'context'       : 30,\n",
        "\n",
        "    'init_lr'       : 1e-3,\n",
        "    'architecture'  : 'Pyramid',\n",
        "    #'dropout'       : 0.15,\n",
        "    # 'betas'         :(0.9, 0.999),\n",
        "    # 'eps'           : 1e-8,\n",
        "    # 'weight_decay'  : 0.1,\n",
        "    # 'amsgrad'       : False\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xi7V8x8W9z4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#TODO: Create a dataset object using the AudioDataset class for the training  data /content/11-785-s24-hw1p2/dev-clean\n",
        "train_data=AudioDataset(root=\"/content/11-785-s24-hw1p2\", context=config['context'], partition = \"train-clean-100\")\n",
        "\n",
        "# TODO: Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = AudioDataset(root= \"/content/11-785-s24-hw1p2\", context=config['context'], partition=\"dev-clean\")\n",
        "\n",
        "# TODO: Create a dataset object using the AudioTestDataset class for the test data\n",
        "test_data = AudioTestDataset(root=\"/content/11-785-s24-hw1p2/test-clean\", context=config['context'], partition=\"test-clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mzoYfTKu14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7866dfb-1107-49aa-b006-c6b6c1cbadd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size     :  2048\n",
            "Context        :  30\n",
            "Input size     :  1647\n",
            "Output symbols :  42\n",
            "Train dataset samples = 36091157, batches = 17623\n",
            "Validation dataset samples = 1928204, batches = 942\n",
            "Test dataset samples = 1934138, batches = 945\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "# We shuffle train dataloader but not val & test dataloader. Why?\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Batch size     : \", config['batch_size'])\n",
        "print(\"Context        : \", config['context'])\n",
        "print(\"Input size     : \", (2*config['context']+1)*27)\n",
        "print(\"Output symbols : \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.length"
      ],
      "metadata": {
        "id": "0a91cqM_uJPD",
        "outputId": "71f6d09b-546a-4f30-84c0-89b29183642d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36091157"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-GV3UvgLSoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85512d7-f8b5-48e2-ae5b-3c11e715decb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2048, 1647]) torch.Size([2048])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcpontXQq9j"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "\n",
        "            # torch.nn.Linear(input_size, 1647),\n",
        "            # torch.nn.BatchNorm1d(1647),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1647, 1647),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1647, 1647),\n",
        "            # torch.nn.BatchNorm1d(1647),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1647, 1500),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1500, 1500),\n",
        "            # torch.nn.BatchNorm1d(1500),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1500, 1500),\n",
        "            # torch.nn.BatchNorm1d(1500),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1500, 1500),\n",
        "            # torch.nn.BatchNorm1d(1500),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(1500, 500),\n",
        "            # torch.nn.BatchNorm1d(500),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(500, 300),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(300, 200),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(200, 100),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p=dropout),\n",
        "            # torch.nn.Linear(100,42),\n",
        "            # torch.nn.GELU(),\n",
        "            # torch.nn.Dropout(p='dropout'),\n",
        "            # torch.nn.Linear(42, output_size)\n",
        "\n",
        "        #     torch.nn.Linear(input_size, 1647),\n",
        "        #     torch.nn.BatchNorm1d(1647),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1647, 1647),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1647, 1647),\n",
        "        #     torch.nn.BatchNorm1d(1647),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1647, 1500),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1500, 1500),\n",
        "        #     torch.nn.BatchNorm1d(1500),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1500, 1500),\n",
        "        #     torch.nn.BatchNorm1d(1500),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1500, 1500),\n",
        "        #     torch.nn.BatchNorm1d(1500),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(1500, 500),\n",
        "        #     torch.nn.BatchNorm1d(500),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(500, 300),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(300, 200),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(200, 100),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(100, 42),\n",
        "        #     torch.nn.GELU(),\n",
        "        #     torch.nn.Dropout(p = config['dropout']),\n",
        "        #     torch.nn.Linear(42, output_size)\n",
        "\n",
        "            torch.nn.Linear(input_size, 1647),\n",
        "            torch.nn.BatchNorm1d(1647),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.2),\n",
        "            torch.nn.Linear(1647, 1647),\n",
        "            torch.nn.BatchNorm1d(1647),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.2),\n",
        "            torch.nn.Linear(1647, 1647),\n",
        "            torch.nn.BatchNorm1d(1647),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.2),\n",
        "            torch.nn.Linear(1647, 1647),\n",
        "            torch.nn.BatchNorm1d(1647),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.2),\n",
        "            torch.nn.Linear(1647, 1500),\n",
        "            torch.nn.BatchNorm1d(1500),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.15),\n",
        "            torch.nn.Linear(1500, 1500),\n",
        "            torch.nn.BatchNorm1d(1500),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.15),\n",
        "            torch.nn.Linear(1500, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.Linear(1000, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.Linear(1000, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(1000, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.Linear(1000, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.Linear(1000, 1000),\n",
        "            torch.nn.BatchNorm1d(1000),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.1),\n",
        "            torch.nn.Linear(1000, 100),\n",
        "            torch.nn.BatchNorm1d(100),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.05),\n",
        "            torch.nn.Linear(100, 42),\n",
        "            torch.nn.BatchNorm1d(42),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(p = 0.05),\n",
        "            torch.nn.Linear(42, output_size)\n",
        "\n",
        "          # torch.nn.Linear(input_size, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.BatchNorm1d(1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Dropout(p = config['dropout']),\n",
        "          #   torch.nn.Linear(1140, 1140),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Linear(1140, 50),\n",
        "          #   torch.nn.BatchNorm1d(50),\n",
        "          #   torch.nn.ReLU(),\n",
        "          #   torch.nn.Linear(50, output_size),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qtrEM1ZvLje",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a38ee6aa-167c-48fd-cda8-22aa132a93be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================\n",
            "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
            "Layer                                                                    \n",
            "0_model.Linear_0         [1647, 1647]  [2048, 1647]  2.714256M  2.712609M\n",
            "1_model.BatchNorm1d_1          [1647]  [2048, 1647]     3.294k     1.647k\n",
            "2_model.GELU_2                      -  [2048, 1647]          -          -\n",
            "3_model.Dropout_3                   -  [2048, 1647]          -          -\n",
            "4_model.Linear_4         [1647, 1647]  [2048, 1647]  2.714256M  2.712609M\n",
            "5_model.BatchNorm1d_5          [1647]  [2048, 1647]     3.294k     1.647k\n",
            "6_model.GELU_6                      -  [2048, 1647]          -          -\n",
            "7_model.Dropout_7                   -  [2048, 1647]          -          -\n",
            "8_model.Linear_8         [1647, 1647]  [2048, 1647]  2.714256M  2.712609M\n",
            "9_model.BatchNorm1d_9          [1647]  [2048, 1647]     3.294k     1.647k\n",
            "10_model.GELU_10                    -  [2048, 1647]          -          -\n",
            "11_model.Dropout_11                 -  [2048, 1647]          -          -\n",
            "12_model.Linear_12       [1647, 1647]  [2048, 1647]  2.714256M  2.712609M\n",
            "13_model.BatchNorm1d_13        [1647]  [2048, 1647]     3.294k     1.647k\n",
            "14_model.GELU_14                    -  [2048, 1647]          -          -\n",
            "15_model.Dropout_15                 -  [2048, 1647]          -          -\n",
            "16_model.Linear_16       [1647, 1500]  [2048, 1500]     2.472M    2.4705M\n",
            "17_model.BatchNorm1d_17        [1500]  [2048, 1500]       3.0k       1.5k\n",
            "18_model.GELU_18                    -  [2048, 1500]          -          -\n",
            "19_model.Dropout_19                 -  [2048, 1500]          -          -\n",
            "20_model.Linear_20       [1500, 1500]  [2048, 1500]    2.2515M      2.25M\n",
            "21_model.BatchNorm1d_21        [1500]  [2048, 1500]       3.0k       1.5k\n",
            "22_model.GELU_22                    -  [2048, 1500]          -          -\n",
            "23_model.Dropout_23                 -  [2048, 1500]          -          -\n",
            "24_model.Linear_24       [1500, 1000]  [2048, 1000]     1.501M       1.5M\n",
            "25_model.BatchNorm1d_25        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "26_model.GELU_26                    -  [2048, 1000]          -          -\n",
            "27_model.Dropout_27                 -  [2048, 1000]          -          -\n",
            "28_model.Linear_28       [1000, 1000]  [2048, 1000]     1.001M       1.0M\n",
            "29_model.BatchNorm1d_29        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "30_model.GELU_30                    -  [2048, 1000]          -          -\n",
            "31_model.Dropout_31                 -  [2048, 1000]          -          -\n",
            "32_model.Linear_32       [1000, 1000]  [2048, 1000]     1.001M       1.0M\n",
            "33_model.BatchNorm1d_33        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "34_model.GELU_34                    -  [2048, 1000]          -          -\n",
            "35_model.Linear_35       [1000, 1000]  [2048, 1000]     1.001M       1.0M\n",
            "36_model.BatchNorm1d_36        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "37_model.Dropout_37                 -  [2048, 1000]          -          -\n",
            "38_model.GELU_38                    -  [2048, 1000]          -          -\n",
            "39_model.Dropout_39                 -  [2048, 1000]          -          -\n",
            "40_model.Linear_40       [1000, 1000]  [2048, 1000]     1.001M       1.0M\n",
            "41_model.BatchNorm1d_41        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "42_model.GELU_42                    -  [2048, 1000]          -          -\n",
            "43_model.Dropout_43                 -  [2048, 1000]          -          -\n",
            "44_model.Linear_44       [1000, 1000]  [2048, 1000]     1.001M       1.0M\n",
            "45_model.BatchNorm1d_45        [1000]  [2048, 1000]       2.0k       1.0k\n",
            "46_model.GELU_46                    -  [2048, 1000]          -          -\n",
            "47_model.Dropout_47                 -  [2048, 1000]          -          -\n",
            "48_model.Linear_48        [1000, 100]   [2048, 100]     100.1k     100.0k\n",
            "49_model.BatchNorm1d_49         [100]   [2048, 100]      200.0      100.0\n",
            "50_model.GELU_50                    -   [2048, 100]          -          -\n",
            "51_model.Dropout_51                 -   [2048, 100]          -          -\n",
            "52_model.Linear_52          [100, 42]    [2048, 42]     4.242k       4.2k\n",
            "53_model.BatchNorm1d_53          [42]    [2048, 42]       84.0       42.0\n",
            "54_model.GELU_54                    -    [2048, 42]          -          -\n",
            "55_model.Dropout_55                 -    [2048, 42]          -          -\n",
            "56_model.Linear_56           [42, 42]    [2048, 42]     1.806k     1.764k\n",
            "-------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          22.224132M\n",
            "Trainable params      22.224132M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds              22.19263M\n",
            "=========================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
              "Layer                                                                    \n",
              "0_model.Linear_0         [1647, 1647]  [2048, 1647]  2714256.0  2712609.0\n",
              "1_model.BatchNorm1d_1          [1647]  [2048, 1647]     3294.0     1647.0\n",
              "2_model.GELU_2                      -  [2048, 1647]        NaN        NaN\n",
              "3_model.Dropout_3                   -  [2048, 1647]        NaN        NaN\n",
              "4_model.Linear_4         [1647, 1647]  [2048, 1647]  2714256.0  2712609.0\n",
              "5_model.BatchNorm1d_5          [1647]  [2048, 1647]     3294.0     1647.0\n",
              "6_model.GELU_6                      -  [2048, 1647]        NaN        NaN\n",
              "7_model.Dropout_7                   -  [2048, 1647]        NaN        NaN\n",
              "8_model.Linear_8         [1647, 1647]  [2048, 1647]  2714256.0  2712609.0\n",
              "9_model.BatchNorm1d_9          [1647]  [2048, 1647]     3294.0     1647.0\n",
              "10_model.GELU_10                    -  [2048, 1647]        NaN        NaN\n",
              "11_model.Dropout_11                 -  [2048, 1647]        NaN        NaN\n",
              "12_model.Linear_12       [1647, 1647]  [2048, 1647]  2714256.0  2712609.0\n",
              "13_model.BatchNorm1d_13        [1647]  [2048, 1647]     3294.0     1647.0\n",
              "14_model.GELU_14                    -  [2048, 1647]        NaN        NaN\n",
              "15_model.Dropout_15                 -  [2048, 1647]        NaN        NaN\n",
              "16_model.Linear_16       [1647, 1500]  [2048, 1500]  2472000.0  2470500.0\n",
              "17_model.BatchNorm1d_17        [1500]  [2048, 1500]     3000.0     1500.0\n",
              "18_model.GELU_18                    -  [2048, 1500]        NaN        NaN\n",
              "19_model.Dropout_19                 -  [2048, 1500]        NaN        NaN\n",
              "20_model.Linear_20       [1500, 1500]  [2048, 1500]  2251500.0  2250000.0\n",
              "21_model.BatchNorm1d_21        [1500]  [2048, 1500]     3000.0     1500.0\n",
              "22_model.GELU_22                    -  [2048, 1500]        NaN        NaN\n",
              "23_model.Dropout_23                 -  [2048, 1500]        NaN        NaN\n",
              "24_model.Linear_24       [1500, 1000]  [2048, 1000]  1501000.0  1500000.0\n",
              "25_model.BatchNorm1d_25        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "26_model.GELU_26                    -  [2048, 1000]        NaN        NaN\n",
              "27_model.Dropout_27                 -  [2048, 1000]        NaN        NaN\n",
              "28_model.Linear_28       [1000, 1000]  [2048, 1000]  1001000.0  1000000.0\n",
              "29_model.BatchNorm1d_29        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "30_model.GELU_30                    -  [2048, 1000]        NaN        NaN\n",
              "31_model.Dropout_31                 -  [2048, 1000]        NaN        NaN\n",
              "32_model.Linear_32       [1000, 1000]  [2048, 1000]  1001000.0  1000000.0\n",
              "33_model.BatchNorm1d_33        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "34_model.GELU_34                    -  [2048, 1000]        NaN        NaN\n",
              "35_model.Linear_35       [1000, 1000]  [2048, 1000]  1001000.0  1000000.0\n",
              "36_model.BatchNorm1d_36        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "37_model.Dropout_37                 -  [2048, 1000]        NaN        NaN\n",
              "38_model.GELU_38                    -  [2048, 1000]        NaN        NaN\n",
              "39_model.Dropout_39                 -  [2048, 1000]        NaN        NaN\n",
              "40_model.Linear_40       [1000, 1000]  [2048, 1000]  1001000.0  1000000.0\n",
              "41_model.BatchNorm1d_41        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "42_model.GELU_42                    -  [2048, 1000]        NaN        NaN\n",
              "43_model.Dropout_43                 -  [2048, 1000]        NaN        NaN\n",
              "44_model.Linear_44       [1000, 1000]  [2048, 1000]  1001000.0  1000000.0\n",
              "45_model.BatchNorm1d_45        [1000]  [2048, 1000]     2000.0     1000.0\n",
              "46_model.GELU_46                    -  [2048, 1000]        NaN        NaN\n",
              "47_model.Dropout_47                 -  [2048, 1000]        NaN        NaN\n",
              "48_model.Linear_48        [1000, 100]   [2048, 100]   100100.0   100000.0\n",
              "49_model.BatchNorm1d_49         [100]   [2048, 100]      200.0      100.0\n",
              "50_model.GELU_50                    -   [2048, 100]        NaN        NaN\n",
              "51_model.Dropout_51                 -   [2048, 100]        NaN        NaN\n",
              "52_model.Linear_52          [100, 42]    [2048, 42]     4242.0     4200.0\n",
              "53_model.BatchNorm1d_53          [42]    [2048, 42]       84.0       42.0\n",
              "54_model.GELU_54                    -    [2048, 42]        NaN        NaN\n",
              "55_model.Dropout_55                 -    [2048, 42]        NaN        NaN\n",
              "56_model.Linear_56           [42, 42]    [2048, 42]     1806.0     1764.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d0dee79-4874-4057-9e6d-efeebc0a7866\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[1647, 1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>2714256.0</td>\n",
              "      <td>2712609.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_model.BatchNorm1d_1</th>\n",
              "      <td>[1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>3294.0</td>\n",
              "      <td>1647.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_model.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_model.Dropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_model.Linear_4</th>\n",
              "      <td>[1647, 1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>2714256.0</td>\n",
              "      <td>2712609.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_model.BatchNorm1d_5</th>\n",
              "      <td>[1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>3294.0</td>\n",
              "      <td>1647.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_model.GELU_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_model.Dropout_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_model.Linear_8</th>\n",
              "      <td>[1647, 1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>2714256.0</td>\n",
              "      <td>2712609.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_model.BatchNorm1d_9</th>\n",
              "      <td>[1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>3294.0</td>\n",
              "      <td>1647.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_model.GELU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_model.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_model.Linear_12</th>\n",
              "      <td>[1647, 1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>2714256.0</td>\n",
              "      <td>2712609.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_model.BatchNorm1d_13</th>\n",
              "      <td>[1647]</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>3294.0</td>\n",
              "      <td>1647.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_model.GELU_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_model.Dropout_15</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1647]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_model.Linear_16</th>\n",
              "      <td>[1647, 1500]</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>2472000.0</td>\n",
              "      <td>2470500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_model.BatchNorm1d_17</th>\n",
              "      <td>[1500]</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>1500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_model.GELU_18</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_model.Dropout_19</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_model.Linear_20</th>\n",
              "      <td>[1500, 1500]</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>2251500.0</td>\n",
              "      <td>2250000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_model.BatchNorm1d_21</th>\n",
              "      <td>[1500]</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>1500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_model.GELU_22</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_model.Dropout_23</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_model.Linear_24</th>\n",
              "      <td>[1500, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1501000.0</td>\n",
              "      <td>1500000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_model.BatchNorm1d_25</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_model.GELU_26</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_model.Dropout_27</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_model.Linear_28</th>\n",
              "      <td>[1000, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1001000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_model.BatchNorm1d_29</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_model.GELU_30</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31_model.Dropout_31</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32_model.Linear_32</th>\n",
              "      <td>[1000, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1001000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33_model.BatchNorm1d_33</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34_model.GELU_34</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35_model.Linear_35</th>\n",
              "      <td>[1000, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1001000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36_model.BatchNorm1d_36</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37_model.Dropout_37</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38_model.GELU_38</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39_model.Dropout_39</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40_model.Linear_40</th>\n",
              "      <td>[1000, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1001000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41_model.BatchNorm1d_41</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42_model.GELU_42</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43_model.Dropout_43</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44_model.Linear_44</th>\n",
              "      <td>[1000, 1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>1001000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45_model.BatchNorm1d_45</th>\n",
              "      <td>[1000]</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46_model.GELU_46</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47_model.Dropout_47</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48_model.Linear_48</th>\n",
              "      <td>[1000, 100]</td>\n",
              "      <td>[2048, 100]</td>\n",
              "      <td>100100.0</td>\n",
              "      <td>100000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49_model.BatchNorm1d_49</th>\n",
              "      <td>[100]</td>\n",
              "      <td>[2048, 100]</td>\n",
              "      <td>200.0</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50_model.GELU_50</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 100]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51_model.Dropout_51</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 100]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52_model.Linear_52</th>\n",
              "      <td>[100, 42]</td>\n",
              "      <td>[2048, 42]</td>\n",
              "      <td>4242.0</td>\n",
              "      <td>4200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53_model.BatchNorm1d_53</th>\n",
              "      <td>[42]</td>\n",
              "      <td>[2048, 42]</td>\n",
              "      <td>84.0</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54_model.GELU_54</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 42]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55_model.Dropout_55</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 42]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56_model.Linear_56</th>\n",
              "      <td>[42, 42]</td>\n",
              "      <td>[2048, 42]</td>\n",
              "      <td>1806.0</td>\n",
              "      <td>1764.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d0dee79-4874-4057-9e6d-efeebc0a7866')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d0dee79-4874-4057-9e6d-efeebc0a7866 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d0dee79-4874-4057-9e6d-efeebc0a7866');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bbfcfa8-6d96-4d1a-9469-0f985b34d08f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bbfcfa8-6d96-4d1a-9469-0f985b34d08f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bbfcfa8-6d96-4d1a-9469-0f985b34d08f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "INPUT_SIZE  = (2*config['context'] + 1) * 27 # Why is this the case?\n",
        "model       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\n",
        "summary(model, frames.to(device))\n",
        "# Check number of parameters of your network\n",
        "# Remember, you are limited to 24 million parameters for HW1 (including ensembles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr'])\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "#optimizer=torch.optim.SGD(model.parameters(), lr=config['init_lr'], momentum=0.5)\n",
        "#scheduler = StepLR(optimizer, step_size=3, gamma = 0.3)\n",
        "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True, min_lr=1e-6)\n",
        "scheduler_cos = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
        "# milestones = [5,10,15, 20,25, 28, 30,35]  # You can adjust the milestones as needed\n",
        "# scheduler_multistep = lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#Defining Optimizer\n",
        "# Recommended : Define Scheduler for Learning Rate,\n",
        "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
        "# You can refer to Pytorch documentation for more information on how to use them.\n",
        "\n",
        "# Is your training time very high?\n",
        "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XblOHEVtKab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975cbe03-988c-4ad9-92d4-dbf4514be010"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, scheduler_cos, scaler):\n",
        "#def train(model, dataloader, optimizer, criterion, scheduler_cos):\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            frames      = frames.to(device)\n",
        "            phonemes    = phonemes.to(device)\n",
        "\n",
        "            logits  = model(frames)\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    #cosine_scheduler.step()\n",
        "    # #reduce_lr_scheduler.step(tloss)\n",
        "    scheduler_cos.step()\n",
        "    # scheduler_multistep.step()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(dataloader)\n",
        "    tacc    /= len(dataloader)\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    #scheduler.step()\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCDYx5VEu6qI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920ee7da-bed1-49c7-ab34-a8cb7cd9abe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "wandb.login(key=\"66a818a0cbd3bb3492e8f908b65fec7b4ed03afc\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvUnYd3Bw2up",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "eccfba5e697c43609e3f2633e8128016",
            "87797df5ff164cfa9cce755176a25d04",
            "e8f45dfbd38b4666a8bcd0ff1f03bf36",
            "295eb181c4fc4bbfb241c6b9141f8b6b",
            "a982f191b49d4d9b9c85508bf73485e0",
            "104e43c991944d3499eed0835ccd5d5c",
            "87b6a898337c49db85a8e753f245f926",
            "b90a89d1f31941d18c05643b5ca222f7"
          ]
        },
        "outputId": "98f218d6-4d48-44be-de76-281c378ca24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marushij\u001b[0m (\u001b[33midl1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111262939998091, max=1.0)…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eccfba5e697c43609e3f2633e8128016"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240210_011900-vf1h3viz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idl1/hw1p2/runs/vf1h3viz' target=\"_blank\">AJ_T13</a></strong> to <a href='https://wandb.ai/idl1/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idl1/hw1p2' target=\"_blank\">https://wandb.ai/idl1/hw1p2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idl1/hw1p2/runs/vf1h3viz' target=\"_blank\">https://wandb.ai/idl1/hw1p2/runs/vf1h3viz</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"AJ_T13\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wft15E_IxYFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab9f8f8-69d0-4783-c6f9-bc0645e48413"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20240210_011900-vf1h3viz/files/model_arch.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model)\n",
        "\n",
        "### Save it in a txt file\n",
        "arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG4F77Nm0Am9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dac59b04-83cd-494f-94c2-909c6b4db167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1022%\tTrain Loss 0.3693\t Learning Rate 0.0004857\n",
            "\tVal Acc 86.0243%\tVal Loss 0.4284\n",
            "\n",
            "Epoch 2/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1417%\tTrain Loss 0.3680\t Learning Rate 0.0004853\n",
            "\tVal Acc 86.0304%\tVal Loss 0.4282\n",
            "\n",
            "Epoch 3/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1475%\tTrain Loss 0.3676\t Learning Rate 0.0004841\n",
            "\tVal Acc 86.0194%\tVal Loss 0.4278\n",
            "\n",
            "Epoch 4/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1590%\tTrain Loss 0.3671\t Learning Rate 0.0004822\n",
            "\tVal Acc 86.0803%\tVal Loss 0.4260\n",
            "\n",
            "Epoch 5/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1647%\tTrain Loss 0.3667\t Learning Rate 0.0004794\n",
            "\tVal Acc 86.1056%\tVal Loss 0.4266\n",
            "\n",
            "Epoch 6/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.1898%\tTrain Loss 0.3660\t Learning Rate 0.0004759\n",
            "\tVal Acc 86.0501%\tVal Loss 0.4267\n",
            "\n",
            "Epoch 7/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.2110%\tTrain Loss 0.3651\t Learning Rate 0.0004716\n",
            "\tVal Acc 86.1222%\tVal Loss 0.4250\n",
            "\n",
            "Epoch 8/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.2430%\tTrain Loss 0.3643\t Learning Rate 0.0004666\n",
            "\tVal Acc 86.1269%\tVal Loss 0.4255\n",
            "\n",
            "Epoch 9/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.2650%\tTrain Loss 0.3634\t Learning Rate 0.0004608\n",
            "\tVal Acc 86.1529%\tVal Loss 0.4240\n",
            "\n",
            "Epoch 10/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Acc 87.3027%\tTrain Loss 0.3621\t Learning Rate 0.0004543\n",
            "\tVal Acc 86.2129%\tVal Loss 0.4245\n",
            "\n",
            "Epoch 11/55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  19%|█▉        | 3330/17623 [02:43<13:00, 18.32it/s, acc=87.4813%, loss=0.3560]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-245200d9caef>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcurr_lr\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_cos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-ee158573b50f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler_cos, scaler)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtacc\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n\u001b[0m\u001b[1;32m     31\u001b[0m                               acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatch_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/redirect.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "scaler = GradScaler()\n",
        "import os\n",
        "\n",
        "# Directory to save the checkpoints\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Path to the checkpoint of the 40th epoch\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"epoch_28.pt\")\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the model state dictionary and optimizer state dictionary\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Get the epoch number from the checkpoint\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion, scheduler_cos, scaler)\n",
        "    val_loss, val_acc       = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "\n",
        "    # Log metrics at each epoch in your run\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'lr': curr_lr\n",
        "    }, checkpoint_path)\n",
        "\n",
        "# Finish your wandb run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/checkpoints/"
      ],
      "metadata": {
        "id": "rGPDF0d19PBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.eval()# TODO train or eval?\n",
        "\n",
        "    ### List to store predicted phonemes of test data\n",
        "    test_predictions = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch.no_grad(): # TODO\n",
        "\n",
        "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            mfccs   = mfccs.to(device)\n",
        "\n",
        "            logits  = model(mfccs)\n",
        "\n",
        "            ### Get most likely predicted phoneme with argmax\n",
        "            predicted_phoneme_indices = torch.argmax(logits, dim = 1)\n",
        "            predicted_phonemes = [PHONEMES[i] for i in predicted_phoneme_indices]\n",
        "\n",
        "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
        "            # TODO\n",
        "            test_predictions.extend(predicted_phonemes)\n",
        "\n",
        "            if i<5:\n",
        "              print([PHONEMES[i] for i in predicted_phoneme_indices])\n",
        "\n",
        "\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"Cyl-Pyr-BatchNorm-SGD\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ],
      "metadata": {
        "id": "H45RZ-bUwG6s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "41826af843da451bade66f42fb750dc8",
            "4ba8aacde30a400e8d69e6f1ef1f48ae",
            "7f840088162243b29a9d006841c01394",
            "9e402bfb24fd4d68ac5decb56c2c881b",
            "43da8606c90e479d896307e8349979e7",
            "b1f53fa7ae1a4e1e8d7751c7482901bb",
            "92f9062fdf5b4f6daeb22e28bed9d673",
            "4659073177ff4346b366661bd74340d8"
          ]
        },
        "outputId": "1e8dc17c-d164-4894-872a-acccc977f0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:vf1h3viz) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41826af843da451bade66f42fb750dc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>███▇▇▆▅▄▂▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▅▄▃▂▁</td></tr><tr><td>val_acc</td><td>▁▁▁▃▄▂▅▅▆█</td></tr><tr><td>valid_loss</td><td>██▇▄▅▅▃▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.00045</td></tr><tr><td>train_acc</td><td>87.30272</td></tr><tr><td>train_loss</td><td>0.36209</td></tr><tr><td>val_acc</td><td>86.21288</td></tr><tr><td>valid_loss</td><td>0.42445</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AJ_T13</strong> at: <a href='https://wandb.ai/idl1/hw1p2/runs/vf1h3viz' target=\"_blank\">https://wandb.ai/idl1/hw1p2/runs/vf1h3viz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240210_011900-vf1h3viz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:vf1h3viz). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240210_035419-tqtf1qn7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idl1/hw1p2/runs/tqtf1qn7' target=\"_blank\">Cyl-Pyr-BatchNorm-SGD</a></strong> to <a href='https://wandb.ai/idl1/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idl1/hw1p2' target=\"_blank\">https://wandb.ai/idl1/hw1p2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idl1/hw1p2/runs/tqtf1qn7' target=\"_blank\">https://wandb.ai/idl1/hw1p2/runs/tqtf1qn7</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG9v6Xmxu7wp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cd105d-488b-42d6-db6b-55bb467bceeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/945 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/945 [00:00<05:14,  3.00it/s]\u001b[A\n",
            "  0%|          | 3/945 [00:00<02:27,  6.38it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'DH', 'DH', 'DH', 'DH', 'DH', 'EH', 'EH', 'EH', 'EH', 'R', 'R', 'R', 'R', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'D', 'D', 'D', 'D', 'D', 'B', 'B', 'B', 'B', 'B', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AO', 'AO', 'R', 'R', 'R', 'R', 'R', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'N', 'N', 'N', 'N', 'N', 'N', 'IH', 'AH', 'IH', 'IH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'EH', 'EH', 'EH', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'B', 'B', 'B', 'B', 'B', 'B', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'Z', 'Z', 'Z', 'Z', 'Z', 'D', 'D', 'D', 'D', 'D', 'D', 'B', 'B', 'B', 'P', 'P', 'P', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'T', 'T', 'T', 'T', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'M', 'M', 'M', 'M', 'M', 'M', 'N', 'N', 'N', 'N', 'N', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'F', 'F', 'F', 'F', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'EH', 'EH', 'EH', 'EH', 'EH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'L', 'L', 'L', 'L', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AH', 'AH', 'AH', 'AH', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', '[SIL]', 'M', 'M', '[SIL]', 'N', 'D', 'D', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'NG', 'NG', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'W', 'W', 'W', 'W', 'W', 'W', 'UH', 'UH', 'UH', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'DH', 'DH', 'DH', 'DH', 'DH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'K', 'K', 'K', 'K', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'JH', 'JH', 'JH', 'JH', 'D', 'D', 'D', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'W', 'W', 'W', 'W', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'R', 'R', 'R', 'R', 'R', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AO', 'AO', 'AO', 'AO', 'AO', 'AA', 'AA', 'AA', 'AA', 'AA', 'AO', 'AO', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]']\n",
            "['[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'AH', 'AH', 'AH', 'HH', 'HH', 'AH', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'AY', 'AY', 'AY', 'ER', 'ER', 'ER', 'ER', 'AY', 'ER', 'ER', 'ER', 'AH', 'AH', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'AH', 'ER', 'ER', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'AH', 'AH', 'EH', 'N', 'N', 'N', 'N', 'D', 'D', 'N', 'N', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'HH', 'Y', 'Y', 'HH', 'HH', 'HH', 'HH', 'R', 'R', 'R', 'R', 'R', 'R', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'T', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AW', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'B', 'B', 'B', 'B', 'B', 'ER', 'ER', 'AH', 'AH', 'ER', 'ER', 'ER', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'R', 'R', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'CH', 'CH', 'CH', 'CH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'EH', 'AW', 'AW', 'AW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'T', 'T', 'T', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'UH', 'UH', 'UH', 'UH', 'UH', 'D', 'D', 'D', 'D', 'N', 'N', 'N', 'N', 'N', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'T', 'T', 'D', 'T', 'D', 'D', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'T', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'IY', 'IY', 'IY', 'IY', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'D', 'D', 'D', 'D', 'D', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'V', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'AE', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'R', 'R', 'R', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AE', 'AE', 'AE', 'AE', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'M', 'M', 'M', 'M', 'M', 'M', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'M', 'M', 'M', 'W', 'M', 'M', 'M', 'M', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AH', 'AH', 'EH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AO', 'AO', 'R', 'R', 'R', 'R', 'R', 'R', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'IH', 'IH', 'IH', 'IH', 'IH', 'IY', 'IY', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'R', 'R', 'R', 'R', 'R', 'R', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'F', 'F', 'F', 'F', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'N', 'N', 'N', 'N', 'T', 'T', 'JH', 'JH', 'JH', 'T', 'T', 'T', 'T', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH']\n",
            "['T', 'JH', 'W', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'EH', 'EH', 'EH', 'EH', 'EH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'R', 'R', 'R', 'R', 'R', 'IY', 'IY', 'IY', 'IY', 'IY', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'B', 'B', 'B', 'B', 'B', 'IH', 'IH', 'IH', 'IH', 'IH', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'D', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'W', 'W', 'W', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'OW', 'OW', 'UH', 'UH', 'UH', 'R', 'R', 'R', 'R', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'T', 'T', 'D', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'R', 'R', 'R', 'R', 'R', 'R', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'D', 'D', 'D', 'D', 'D', 'AH', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'V', 'B', 'B', 'B', 'B', 'B', 'B', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'D', 'D', 'D', 'D', 'D', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', '[SIL]', '[SIL]', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', '[SIL]', '[SIL]', '[SIL]', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'UW', 'UW', 'UW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'ER', 'ER', 'ER', 'ER', 'ER', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'W', 'W', 'R', 'R', 'R', 'R', 'R', 'ER', 'ER', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'R', 'R', 'R', 'R', 'R', 'R', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'G', 'G', 'G', 'G', 'G', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'T', 'T', 'T', 'T', 'T', 'T', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AA', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'K', '[SIL]', '[SIL]', '[SIL]', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'OW', 'OW', 'OW', 'OW', 'OW', 'UW', 'UW', 'UW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'UW', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'ER', 'ER', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'T', 'T', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'M', 'M', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AH', 'AH', 'AH', 'IH', 'IH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'IH', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'D', 'D', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AE', 'AE', 'AE', 'AE', 'AE', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'L', 'L', 'L', 'L', 'L', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'V', 'V', 'V', 'V', 'AH', 'AH']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  1%|          | 5/945 [00:00<01:56,  8.10it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'AH', 'AH', 'AH', 'AH', 'AH', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'B', 'B', 'B', 'B', 'B', 'B', 'L', 'L', 'L', 'L', 'L', 'L', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'L', 'L', 'L', 'L', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AH', 'AH', 'ER', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AO', 'R', 'R', 'R', 'R', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ZH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'AH', 'AH', 'UW', 'UW', 'UW', 'UW', 'UW', 'W', 'W', 'W', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'L', 'L', 'L', 'L', 'L', 'L', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'AE', 'AE', 'AE', 'AE', 'AE', 'V', 'V', 'V', 'V', 'V', 'V', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'D', 'D', 'D', 'D', 'D', 'IH', 'IY', 'IY', 'IY', 'IY', 'IY', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'R', 'R', 'R', 'R', 'N', 'N', 'N', 'N', 'N', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'EH', 'EH', 'EH', 'EH', 'N', 'N', 'N', 'N', 'N', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'AW', 'L', 'L', 'L', 'L', 'L', 'AW', 'AW', 'AH', 'AH', 'AH', 'AH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'IH', 'IH', 'IH', 'IH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'T', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'L', 'L', 'L', 'L', 'L', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AA', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'UH', 'UH', 'UH', 'UH', 'UH', 'UH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'L', 'L', 'L', 'L', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'AE', 'AE', 'AE', 'AE', 'T', 'T', 'T', 'T', 'T', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH']\n",
            "['AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'ER', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'V', 'V', 'V', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'AO', 'L', 'L', 'L', 'L', 'L', 'L', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'D', 'D', 'D', 'D', 'D', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'OY', 'S', 'S', 'S', 'S', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'TH', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'R', 'R', 'R', 'R', 'R', 'R', 'IH', 'IY', 'IY', 'IY', 'IY', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'D', 'D', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D', 'D', 'D', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'W', 'W', 'W', 'W', 'W', 'W', 'IH', 'IH', 'IH', 'IH', 'IH', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'IH', 'IH', 'IH', 'IH', 'IH', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'L', 'IH', 'IH', 'IY', 'IY', 'IY', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'JH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'AY', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'L', 'L', 'L', 'L', 'L', 'L', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'IH', 'IH', 'IH', 'IH', 'IH', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'NG', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'EY', 'EY', 'IH', 'IH', 'IH', 'NG', 'EY', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'N', 'N', 'N', 'N', 'N', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'L', 'L', 'L', 'L', 'L', 'L', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', 'SH', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'DH', 'DH', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'IH', 'IH', 'IH', 'IH', 'IH', 'D', 'D', 'D', 'D', 'D', 'D', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'T', 'T', 'T', 'T', 'T', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'Y', 'Y', 'Y', 'UW', 'UW', 'UW', 'UW', 'UW', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'IH', 'IH', 'IH', 'IH', 'IH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'IY', 'IY', 'Y', 'Y', 'Y', 'Y', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'EY', 'T', 'K', 'K', 'K', 'T', 'T', 'T', 'T', 'T', 'DH', 'DH', 'DH', 'DH', 'DH', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'AH', 'AH', 'AH', 'AH', 'V', 'V', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'R', 'R', 'R', 'R', 'R', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'OW', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'CH', 'T', 'T', 'T', 'T', 'T', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'V', 'ER', 'ER', 'ER', 'ER', 'ER', 'ER', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'IY', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'AA', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'Z', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'D', 'D', 'D', 'D', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'K', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'AE', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'R', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'N', 'N', 'N', 'N', 'N', 'N', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'UW', 'UW', 'UW', 'UW', 'UW', 'UW', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'IH', 'IY', 'IH', 'IH', 'IH', 'IH', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'EH', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]', '[SIL]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  1%|          | 7/945 [00:00<01:31, 10.22it/s]\u001b[A\n",
            "  1%|          | 9/945 [00:00<01:17, 12.06it/s]\u001b[A\n",
            "  1%|          | 11/945 [00:01<01:09, 13.46it/s]\u001b[A\n",
            "  1%|▏         | 13/945 [00:01<01:03, 14.77it/s]\u001b[A\n",
            "  2%|▏         | 15/945 [00:01<00:58, 15.98it/s]\u001b[A\n",
            "  2%|▏         | 18/945 [00:01<00:52, 17.67it/s]\u001b[A\n",
            "  2%|▏         | 21/945 [00:01<00:49, 18.81it/s]\u001b[A\n",
            "  3%|▎         | 24/945 [00:01<00:47, 19.31it/s]\u001b[A\n",
            "  3%|▎         | 27/945 [00:01<00:46, 19.65it/s]\u001b[A\n",
            "  3%|▎         | 29/945 [00:01<00:46, 19.66it/s]\u001b[A\n",
            "  3%|▎         | 32/945 [00:02<00:45, 20.16it/s]\u001b[A\n",
            "  4%|▎         | 35/945 [00:02<00:44, 20.51it/s]\u001b[A\n",
            "  4%|▍         | 38/945 [00:02<00:44, 20.25it/s]\u001b[A\n",
            "  4%|▍         | 41/945 [00:02<00:45, 19.90it/s]\u001b[A\n",
            "  5%|▍         | 44/945 [00:02<00:44, 20.34it/s]\u001b[A\n",
            "  5%|▍         | 47/945 [00:02<00:43, 20.47it/s]\u001b[A\n",
            "  5%|▌         | 50/945 [00:02<00:42, 20.85it/s]\u001b[A\n",
            "  6%|▌         | 53/945 [00:03<00:42, 21.09it/s]\u001b[A\n",
            "  6%|▌         | 56/945 [00:03<00:42, 21.10it/s]\u001b[A\n",
            "  6%|▌         | 59/945 [00:03<00:41, 21.27it/s]\u001b[A\n",
            "  7%|▋         | 62/945 [00:03<00:41, 21.12it/s]\u001b[A\n",
            "  7%|▋         | 65/945 [00:03<00:41, 21.32it/s]\u001b[A\n",
            "  7%|▋         | 68/945 [00:03<00:41, 21.30it/s]\u001b[A\n",
            "  8%|▊         | 71/945 [00:03<00:40, 21.37it/s]\u001b[A\n",
            "  8%|▊         | 74/945 [00:04<00:41, 21.16it/s]\u001b[A\n",
            "  8%|▊         | 77/945 [00:04<00:41, 20.91it/s]\u001b[A\n",
            "  8%|▊         | 80/945 [00:04<00:41, 20.72it/s]\u001b[A\n",
            "  9%|▉         | 83/945 [00:04<00:41, 20.83it/s]\u001b[A\n",
            "  9%|▉         | 86/945 [00:04<00:41, 20.60it/s]\u001b[A\n",
            "  9%|▉         | 89/945 [00:04<00:42, 20.26it/s]\u001b[A\n",
            " 10%|▉         | 92/945 [00:04<00:42, 20.20it/s]\u001b[A\n",
            " 10%|█         | 95/945 [00:05<00:41, 20.40it/s]\u001b[A\n",
            " 10%|█         | 98/945 [00:05<00:41, 20.64it/s]\u001b[A\n",
            " 11%|█         | 101/945 [00:05<00:40, 20.87it/s]\u001b[A\n",
            " 11%|█         | 104/945 [00:05<00:39, 21.08it/s]\u001b[A\n",
            " 11%|█▏        | 107/945 [00:05<00:39, 20.95it/s]\u001b[A\n",
            " 12%|█▏        | 110/945 [00:05<00:40, 20.37it/s]\u001b[A\n",
            " 12%|█▏        | 113/945 [00:05<00:40, 20.63it/s]\u001b[A\n",
            " 12%|█▏        | 116/945 [00:06<00:39, 20.85it/s]\u001b[A\n",
            " 13%|█▎        | 119/945 [00:06<00:39, 21.11it/s]\u001b[A\n",
            " 13%|█▎        | 122/945 [00:06<00:39, 20.68it/s]\u001b[A\n",
            " 13%|█▎        | 125/945 [00:06<00:39, 20.64it/s]\u001b[A\n",
            " 14%|█▎        | 128/945 [00:06<00:39, 20.85it/s]\u001b[A\n",
            " 14%|█▍        | 131/945 [00:06<00:38, 20.98it/s]\u001b[A\n",
            " 14%|█▍        | 134/945 [00:07<00:38, 21.00it/s]\u001b[A\n",
            " 14%|█▍        | 137/945 [00:07<00:38, 20.89it/s]\u001b[A\n",
            " 15%|█▍        | 140/945 [00:07<00:38, 20.84it/s]\u001b[A\n",
            " 15%|█▌        | 143/945 [00:07<00:38, 20.97it/s]\u001b[A\n",
            " 15%|█▌        | 146/945 [00:07<00:37, 21.06it/s]\u001b[A\n",
            " 16%|█▌        | 149/945 [00:07<00:37, 21.16it/s]\u001b[A\n",
            " 16%|█▌        | 152/945 [00:07<00:37, 21.04it/s]\u001b[A\n",
            " 16%|█▋        | 155/945 [00:07<00:37, 21.01it/s]\u001b[A\n",
            " 17%|█▋        | 158/945 [00:08<00:37, 21.08it/s]\u001b[A\n",
            " 17%|█▋        | 161/945 [00:08<00:37, 21.04it/s]\u001b[A\n",
            " 17%|█▋        | 164/945 [00:08<00:37, 20.69it/s]\u001b[A\n",
            " 18%|█▊        | 167/945 [00:08<00:37, 20.58it/s]\u001b[A\n",
            " 18%|█▊        | 170/945 [00:08<00:37, 20.71it/s]\u001b[A\n",
            " 18%|█▊        | 173/945 [00:08<00:37, 20.71it/s]\u001b[A\n",
            " 19%|█▊        | 176/945 [00:09<00:36, 20.87it/s]\u001b[A\n",
            " 19%|█▉        | 179/945 [00:09<00:36, 20.97it/s]\u001b[A\n",
            " 19%|█▉        | 182/945 [00:09<00:36, 20.91it/s]\u001b[A\n",
            " 20%|█▉        | 185/945 [00:09<00:36, 21.04it/s]\u001b[A\n",
            " 20%|█▉        | 188/945 [00:09<00:36, 20.81it/s]\u001b[A\n",
            " 20%|██        | 191/945 [00:09<00:35, 20.98it/s]\u001b[A\n",
            " 21%|██        | 194/945 [00:09<00:35, 21.01it/s]\u001b[A\n",
            " 21%|██        | 197/945 [00:10<00:35, 20.83it/s]\u001b[A\n",
            " 21%|██        | 200/945 [00:10<00:35, 20.87it/s]\u001b[A\n",
            " 21%|██▏       | 203/945 [00:10<00:35, 21.14it/s]\u001b[A\n",
            " 22%|██▏       | 206/945 [00:10<00:34, 21.29it/s]\u001b[A\n",
            " 22%|██▏       | 209/945 [00:10<00:34, 21.25it/s]\u001b[A\n",
            " 22%|██▏       | 212/945 [00:10<00:34, 21.18it/s]\u001b[A\n",
            " 23%|██▎       | 215/945 [00:10<00:34, 21.03it/s]\u001b[A\n",
            " 23%|██▎       | 218/945 [00:11<00:34, 21.21it/s]\u001b[A\n",
            " 23%|██▎       | 221/945 [00:11<00:34, 20.90it/s]\u001b[A\n",
            " 24%|██▎       | 224/945 [00:11<00:34, 21.07it/s]\u001b[A\n",
            " 24%|██▍       | 227/945 [00:11<00:34, 21.00it/s]\u001b[A\n",
            " 24%|██▍       | 230/945 [00:11<00:33, 21.23it/s]\u001b[A\n",
            " 25%|██▍       | 233/945 [00:11<00:33, 21.31it/s]\u001b[A\n",
            " 25%|██▍       | 236/945 [00:11<00:33, 21.29it/s]\u001b[A\n",
            " 25%|██▌       | 239/945 [00:11<00:33, 21.20it/s]\u001b[A\n",
            " 26%|██▌       | 242/945 [00:12<00:34, 20.50it/s]\u001b[A\n",
            " 26%|██▌       | 245/945 [00:12<00:33, 20.63it/s]\u001b[A\n",
            " 26%|██▌       | 248/945 [00:12<00:33, 20.88it/s]\u001b[A\n",
            " 27%|██▋       | 251/945 [00:12<00:33, 20.85it/s]\u001b[A\n",
            " 27%|██▋       | 254/945 [00:12<00:33, 20.78it/s]\u001b[A\n",
            " 27%|██▋       | 257/945 [00:12<00:33, 20.31it/s]\u001b[A\n",
            " 28%|██▊       | 260/945 [00:13<00:33, 20.48it/s]\u001b[A\n",
            " 28%|██▊       | 263/945 [00:13<00:33, 20.61it/s]\u001b[A\n",
            " 28%|██▊       | 266/945 [00:13<00:32, 20.76it/s]\u001b[A\n",
            " 28%|██▊       | 269/945 [00:13<00:32, 20.67it/s]\u001b[A\n",
            " 29%|██▉       | 272/945 [00:13<00:32, 20.93it/s]\u001b[A\n",
            " 29%|██▉       | 275/945 [00:13<00:31, 20.97it/s]\u001b[A\n",
            " 29%|██▉       | 278/945 [00:13<00:32, 20.77it/s]\u001b[A\n",
            " 30%|██▉       | 281/945 [00:14<00:32, 20.69it/s]\u001b[A\n",
            " 30%|███       | 284/945 [00:14<00:31, 20.97it/s]\u001b[A\n",
            " 30%|███       | 287/945 [00:14<00:31, 21.20it/s]\u001b[A\n",
            " 31%|███       | 290/945 [00:14<00:30, 21.35it/s]\u001b[A\n",
            " 31%|███       | 293/945 [00:14<00:30, 21.35it/s]\u001b[A\n",
            " 31%|███▏      | 296/945 [00:14<00:31, 20.64it/s]\u001b[A\n",
            " 32%|███▏      | 299/945 [00:14<00:31, 20.53it/s]\u001b[A\n",
            " 32%|███▏      | 302/945 [00:15<00:30, 20.74it/s]\u001b[A\n",
            " 32%|███▏      | 305/945 [00:15<00:30, 20.89it/s]\u001b[A\n",
            " 33%|███▎      | 308/945 [00:15<00:30, 20.92it/s]\u001b[A\n",
            " 33%|███▎      | 311/945 [00:15<00:30, 21.04it/s]\u001b[A\n",
            " 33%|███▎      | 314/945 [00:15<00:30, 21.03it/s]\u001b[A\n",
            " 34%|███▎      | 317/945 [00:15<00:29, 21.02it/s]\u001b[A\n",
            " 34%|███▍      | 320/945 [00:15<00:29, 21.10it/s]\u001b[A\n",
            " 34%|███▍      | 323/945 [00:16<00:29, 21.01it/s]\u001b[A\n",
            " 34%|███▍      | 326/945 [00:16<00:29, 21.09it/s]\u001b[A\n",
            " 35%|███▍      | 329/945 [00:16<00:29, 21.05it/s]\u001b[A\n",
            " 35%|███▌      | 332/945 [00:16<00:29, 20.86it/s]\u001b[A\n",
            " 35%|███▌      | 335/945 [00:16<00:29, 20.91it/s]\u001b[A\n",
            " 36%|███▌      | 338/945 [00:16<00:29, 20.85it/s]\u001b[A\n",
            " 36%|███▌      | 341/945 [00:16<00:28, 20.89it/s]\u001b[A\n",
            " 36%|███▋      | 344/945 [00:17<00:29, 20.69it/s]\u001b[A\n",
            " 37%|███▋      | 347/945 [00:17<00:29, 20.48it/s]\u001b[A\n",
            " 37%|███▋      | 350/945 [00:17<00:29, 20.48it/s]\u001b[A\n",
            " 37%|███▋      | 353/945 [00:17<00:28, 20.78it/s]\u001b[A\n",
            " 38%|███▊      | 356/945 [00:17<00:28, 20.76it/s]\u001b[A\n",
            " 38%|███▊      | 359/945 [00:17<00:28, 20.68it/s]\u001b[A\n",
            " 38%|███▊      | 362/945 [00:17<00:27, 20.90it/s]\u001b[A\n",
            " 39%|███▊      | 365/945 [00:18<00:27, 20.84it/s]\u001b[A\n",
            " 39%|███▉      | 368/945 [00:18<00:27, 20.75it/s]\u001b[A\n",
            " 39%|███▉      | 371/945 [00:18<00:43, 13.08it/s]\u001b[A\n",
            " 40%|███▉      | 374/945 [00:18<00:38, 14.71it/s]\u001b[A\n",
            " 40%|███▉      | 377/945 [00:18<00:34, 16.23it/s]\u001b[A\n",
            " 40%|████      | 380/945 [00:19<00:32, 17.47it/s]\u001b[A\n",
            " 40%|████      | 382/945 [00:19<00:31, 17.97it/s]\u001b[A\n",
            " 41%|████      | 385/945 [00:19<00:29, 18.85it/s]\u001b[A\n",
            " 41%|████      | 388/945 [00:19<00:29, 19.14it/s]\u001b[A\n",
            " 41%|████▏     | 391/945 [00:19<00:28, 19.55it/s]\u001b[A\n",
            " 42%|████▏     | 394/945 [00:19<00:27, 20.04it/s]\u001b[A\n",
            " 42%|████▏     | 397/945 [00:19<00:27, 20.15it/s]\u001b[A\n",
            " 42%|████▏     | 400/945 [00:20<00:26, 20.46it/s]\u001b[A\n",
            " 43%|████▎     | 403/945 [00:20<00:26, 20.42it/s]\u001b[A\n",
            " 43%|████▎     | 406/945 [00:20<00:26, 20.60it/s]\u001b[A\n",
            " 43%|████▎     | 409/945 [00:20<00:25, 20.80it/s]\u001b[A\n",
            " 44%|████▎     | 412/945 [00:20<00:25, 20.88it/s]\u001b[A\n",
            " 44%|████▍     | 415/945 [00:20<00:25, 20.62it/s]\u001b[A\n",
            " 44%|████▍     | 418/945 [00:20<00:25, 20.48it/s]\u001b[A\n",
            " 45%|████▍     | 421/945 [00:21<00:25, 20.73it/s]\u001b[A\n",
            " 45%|████▍     | 424/945 [00:21<00:25, 20.75it/s]\u001b[A\n",
            " 45%|████▌     | 427/945 [00:21<00:24, 20.82it/s]\u001b[A\n",
            " 46%|████▌     | 430/945 [00:21<00:24, 20.94it/s]\u001b[A\n",
            " 46%|████▌     | 433/945 [00:21<00:24, 21.02it/s]\u001b[A\n",
            " 46%|████▌     | 436/945 [00:21<00:24, 21.09it/s]\u001b[A\n",
            " 46%|████▋     | 439/945 [00:21<00:24, 21.08it/s]\u001b[A\n",
            " 47%|████▋     | 442/945 [00:22<00:23, 21.07it/s]\u001b[A\n",
            " 47%|████▋     | 445/945 [00:22<00:24, 20.61it/s]\u001b[A\n",
            " 47%|████▋     | 448/945 [00:22<00:24, 20.47it/s]\u001b[A\n",
            " 48%|████▊     | 451/945 [00:22<00:24, 20.58it/s]\u001b[A\n",
            " 48%|████▊     | 454/945 [00:22<00:25, 19.52it/s]\u001b[A\n",
            " 48%|████▊     | 456/945 [00:22<00:25, 19.49it/s]\u001b[A\n",
            " 48%|████▊     | 458/945 [00:22<00:24, 19.48it/s]\u001b[A\n",
            " 49%|████▊     | 460/945 [00:22<00:24, 19.60it/s]\u001b[A\n",
            " 49%|████▉     | 463/945 [00:23<00:24, 19.96it/s]\u001b[A\n",
            " 49%|████▉     | 466/945 [00:23<00:23, 20.45it/s]\u001b[A\n",
            " 50%|████▉     | 469/945 [00:23<00:23, 20.53it/s]\u001b[A\n",
            " 50%|████▉     | 472/945 [00:23<00:22, 20.75it/s]\u001b[A\n",
            " 50%|█████     | 475/945 [00:23<00:22, 20.80it/s]\u001b[A\n",
            " 51%|█████     | 478/945 [00:23<00:22, 20.93it/s]\u001b[A\n",
            " 51%|█████     | 481/945 [00:23<00:22, 21.00it/s]\u001b[A\n",
            " 51%|█████     | 484/945 [00:24<00:21, 21.00it/s]\u001b[A\n",
            " 52%|█████▏    | 487/945 [00:24<00:21, 21.14it/s]\u001b[A\n",
            " 52%|█████▏    | 490/945 [00:24<00:21, 20.93it/s]\u001b[A\n",
            " 52%|█████▏    | 493/945 [00:24<00:21, 20.79it/s]\u001b[A\n",
            " 52%|█████▏    | 496/945 [00:24<00:21, 20.90it/s]\u001b[A\n",
            " 53%|█████▎    | 499/945 [00:24<00:21, 20.77it/s]\u001b[A\n",
            " 53%|█████▎    | 502/945 [00:24<00:21, 20.59it/s]\u001b[A\n",
            " 53%|█████▎    | 505/945 [00:25<00:21, 20.65it/s]\u001b[A\n",
            " 54%|█████▍    | 508/945 [00:25<00:21, 20.50it/s]\u001b[A\n",
            " 54%|█████▍    | 511/945 [00:25<00:21, 19.96it/s]\u001b[A\n",
            " 54%|█████▍    | 514/945 [00:25<00:21, 20.20it/s]\u001b[A\n",
            " 55%|█████▍    | 517/945 [00:25<00:20, 20.45it/s]\u001b[A\n",
            " 55%|█████▌    | 520/945 [00:25<00:20, 20.74it/s]\u001b[A\n",
            " 55%|█████▌    | 523/945 [00:25<00:20, 20.68it/s]\u001b[A\n",
            " 56%|█████▌    | 526/945 [00:26<00:20, 20.67it/s]\u001b[A\n",
            " 56%|█████▌    | 529/945 [00:26<00:20, 20.72it/s]\u001b[A\n",
            " 56%|█████▋    | 532/945 [00:26<00:19, 20.75it/s]\u001b[A\n",
            " 57%|█████▋    | 535/945 [00:26<00:20, 20.49it/s]\u001b[A\n",
            " 57%|█████▋    | 538/945 [00:26<00:19, 20.39it/s]\u001b[A\n",
            " 57%|█████▋    | 541/945 [00:26<00:19, 20.57it/s]\u001b[A\n",
            " 58%|█████▊    | 544/945 [00:27<00:19, 20.69it/s]\u001b[A\n",
            " 58%|█████▊    | 547/945 [00:27<00:19, 20.68it/s]\u001b[A\n",
            " 58%|█████▊    | 550/945 [00:27<00:19, 20.69it/s]\u001b[A\n",
            " 59%|█████▊    | 553/945 [00:27<00:18, 20.96it/s]\u001b[A\n",
            " 59%|█████▉    | 556/945 [00:27<00:18, 20.76it/s]\u001b[A\n",
            " 59%|█████▉    | 559/945 [00:27<00:18, 20.47it/s]\u001b[A\n",
            " 59%|█████▉    | 562/945 [00:27<00:18, 20.43it/s]\u001b[A\n",
            " 60%|█████▉    | 565/945 [00:28<00:18, 20.68it/s]\u001b[A\n",
            " 60%|██████    | 568/945 [00:28<00:18, 20.81it/s]\u001b[A\n",
            " 60%|██████    | 571/945 [00:28<00:17, 21.06it/s]\u001b[A\n",
            " 61%|██████    | 574/945 [00:28<00:18, 20.45it/s]\u001b[A\n",
            " 61%|██████    | 577/945 [00:28<00:18, 20.39it/s]\u001b[A\n",
            " 61%|██████▏   | 580/945 [00:28<00:18, 20.24it/s]\u001b[A\n",
            " 62%|██████▏   | 583/945 [00:29<00:24, 14.64it/s]\u001b[A\n",
            " 62%|██████▏   | 586/945 [00:29<00:22, 16.07it/s]\u001b[A\n",
            " 62%|██████▏   | 589/945 [00:29<00:20, 17.35it/s]\u001b[A\n",
            " 63%|██████▎   | 592/945 [00:29<00:19, 18.30it/s]\u001b[A\n",
            " 63%|██████▎   | 595/945 [00:29<00:18, 18.88it/s]\u001b[A\n",
            " 63%|██████▎   | 598/945 [00:29<00:17, 19.51it/s]\u001b[A\n",
            " 64%|██████▎   | 601/945 [00:29<00:17, 19.95it/s]\u001b[A\n",
            " 64%|██████▍   | 604/945 [00:30<00:16, 20.13it/s]\u001b[A\n",
            " 64%|██████▍   | 607/945 [00:30<00:16, 20.35it/s]\u001b[A\n",
            " 65%|██████▍   | 610/945 [00:30<00:16, 20.65it/s]\u001b[A\n",
            " 65%|██████▍   | 613/945 [00:30<00:15, 20.77it/s]\u001b[A\n",
            " 65%|██████▌   | 616/945 [00:30<00:15, 20.57it/s]\u001b[A\n",
            " 66%|██████▌   | 619/945 [00:30<00:15, 20.38it/s]\u001b[A\n",
            " 66%|██████▌   | 622/945 [00:30<00:15, 20.31it/s]\u001b[A\n",
            " 66%|██████▌   | 625/945 [00:31<00:15, 20.50it/s]\u001b[A\n",
            " 66%|██████▋   | 628/945 [00:31<00:15, 20.78it/s]\u001b[A\n",
            " 67%|██████▋   | 631/945 [00:31<00:15, 20.78it/s]\u001b[A\n",
            " 67%|██████▋   | 634/945 [00:31<00:14, 20.77it/s]\u001b[A\n",
            " 67%|██████▋   | 637/945 [00:31<00:14, 21.01it/s]\u001b[A\n",
            " 68%|██████▊   | 640/945 [00:31<00:14, 20.79it/s]\u001b[A\n",
            " 68%|██████▊   | 643/945 [00:31<00:14, 20.69it/s]\u001b[A\n",
            " 68%|██████▊   | 646/945 [00:32<00:14, 20.21it/s]\u001b[A\n",
            " 69%|██████▊   | 649/945 [00:32<00:14, 20.52it/s]\u001b[A\n",
            " 69%|██████▉   | 652/945 [00:32<00:14, 20.35it/s]\u001b[A\n",
            " 69%|██████▉   | 655/945 [00:32<00:14, 20.44it/s]\u001b[A\n",
            " 70%|██████▉   | 658/945 [00:32<00:13, 20.63it/s]\u001b[A\n",
            " 70%|██████▉   | 661/945 [00:32<00:14, 20.20it/s]\u001b[A\n",
            " 70%|███████   | 664/945 [00:33<00:14, 20.05it/s]\u001b[A\n",
            " 71%|███████   | 667/945 [00:33<00:13, 20.19it/s]\u001b[A\n",
            " 71%|███████   | 670/945 [00:33<00:13, 20.13it/s]\u001b[A\n",
            " 71%|███████   | 673/945 [00:33<00:13, 20.27it/s]\u001b[A\n",
            " 72%|███████▏  | 676/945 [00:33<00:13, 20.35it/s]\u001b[A\n",
            " 72%|███████▏  | 679/945 [00:33<00:13, 20.42it/s]\u001b[A\n",
            " 72%|███████▏  | 682/945 [00:33<00:12, 20.41it/s]\u001b[A\n",
            " 72%|███████▏  | 685/945 [00:34<00:12, 20.75it/s]\u001b[A\n",
            " 73%|███████▎  | 688/945 [00:34<00:12, 20.85it/s]\u001b[A\n",
            " 73%|███████▎  | 691/945 [00:34<00:12, 21.01it/s]\u001b[A\n",
            " 73%|███████▎  | 694/945 [00:34<00:12, 20.71it/s]\u001b[A\n",
            " 74%|███████▍  | 697/945 [00:34<00:12, 20.58it/s]\u001b[A\n",
            " 74%|███████▍  | 700/945 [00:34<00:11, 20.42it/s]\u001b[A\n",
            " 74%|███████▍  | 703/945 [00:34<00:11, 20.54it/s]\u001b[A\n",
            " 75%|███████▍  | 706/945 [00:35<00:11, 20.43it/s]\u001b[A\n",
            " 75%|███████▌  | 709/945 [00:35<00:11, 20.69it/s]\u001b[A\n",
            " 75%|███████▌  | 712/945 [00:35<00:11, 20.78it/s]\u001b[A\n",
            " 76%|███████▌  | 715/945 [00:35<00:11, 20.63it/s]\u001b[A\n",
            " 76%|███████▌  | 718/945 [00:35<00:10, 20.69it/s]\u001b[A\n",
            " 76%|███████▋  | 721/945 [00:35<00:10, 20.59it/s]\u001b[A\n",
            " 77%|███████▋  | 724/945 [00:35<00:10, 20.70it/s]\u001b[A\n",
            " 77%|███████▋  | 727/945 [00:36<00:10, 20.80it/s]\u001b[A\n",
            " 77%|███████▋  | 730/945 [00:36<00:10, 20.76it/s]\u001b[A\n",
            " 78%|███████▊  | 733/945 [00:36<00:10, 20.61it/s]\u001b[A\n",
            " 78%|███████▊  | 736/945 [00:36<00:10, 20.81it/s]\u001b[A\n",
            " 78%|███████▊  | 739/945 [00:36<00:09, 20.72it/s]\u001b[A\n",
            " 79%|███████▊  | 742/945 [00:36<00:09, 20.93it/s]\u001b[A\n",
            " 79%|███████▉  | 745/945 [00:36<00:09, 20.81it/s]\u001b[A\n",
            " 79%|███████▉  | 748/945 [00:37<00:09, 20.78it/s]\u001b[A\n",
            " 79%|███████▉  | 751/945 [00:37<00:09, 20.89it/s]\u001b[A\n",
            " 80%|███████▉  | 754/945 [00:37<00:09, 21.00it/s]\u001b[A\n",
            " 80%|████████  | 757/945 [00:37<00:08, 20.90it/s]\u001b[A\n",
            " 80%|████████  | 760/945 [00:37<00:08, 20.96it/s]\u001b[A\n",
            " 81%|████████  | 763/945 [00:37<00:08, 20.62it/s]\u001b[A\n",
            " 81%|████████  | 766/945 [00:37<00:08, 20.75it/s]\u001b[A\n",
            " 81%|████████▏ | 769/945 [00:38<00:08, 20.68it/s]\u001b[A\n",
            " 82%|████████▏ | 772/945 [00:38<00:08, 20.78it/s]\u001b[A\n",
            " 82%|████████▏ | 775/945 [00:38<00:08, 20.95it/s]\u001b[A\n",
            " 82%|████████▏ | 778/945 [00:38<00:07, 21.14it/s]\u001b[A\n",
            " 83%|████████▎ | 781/945 [00:38<00:07, 21.03it/s]\u001b[A\n",
            " 83%|████████▎ | 784/945 [00:38<00:07, 20.61it/s]\u001b[A\n",
            " 83%|████████▎ | 787/945 [00:38<00:07, 20.31it/s]\u001b[A\n",
            " 84%|████████▎ | 790/945 [00:39<00:07, 20.51it/s]\u001b[A\n",
            " 84%|████████▍ | 793/945 [00:39<00:07, 20.48it/s]\u001b[A\n",
            " 84%|████████▍ | 796/945 [00:39<00:07, 20.53it/s]\u001b[A\n",
            " 85%|████████▍ | 799/945 [00:39<00:07, 20.36it/s]\u001b[A\n",
            " 85%|████████▍ | 802/945 [00:39<00:07, 20.36it/s]\u001b[A\n",
            " 85%|████████▌ | 805/945 [00:39<00:06, 20.47it/s]\u001b[A\n",
            " 86%|████████▌ | 808/945 [00:39<00:06, 20.46it/s]\u001b[A\n",
            " 86%|████████▌ | 811/945 [00:40<00:06, 20.54it/s]\u001b[A\n",
            " 86%|████████▌ | 814/945 [00:40<00:06, 20.40it/s]\u001b[A\n",
            " 86%|████████▋ | 817/945 [00:40<00:06, 20.64it/s]\u001b[A\n",
            " 87%|████████▋ | 820/945 [00:40<00:05, 20.85it/s]\u001b[A\n",
            " 87%|████████▋ | 823/945 [00:40<00:05, 20.95it/s]\u001b[A\n",
            " 87%|████████▋ | 826/945 [00:40<00:05, 20.64it/s]\u001b[A\n",
            " 88%|████████▊ | 829/945 [00:41<00:05, 20.71it/s]\u001b[A\n",
            " 88%|████████▊ | 832/945 [00:41<00:05, 20.85it/s]\u001b[A\n",
            " 88%|████████▊ | 835/945 [00:41<00:05, 20.90it/s]\u001b[A\n",
            " 89%|████████▊ | 838/945 [00:41<00:05, 21.00it/s]\u001b[A\n",
            " 89%|████████▉ | 841/945 [00:41<00:04, 21.09it/s]\u001b[A\n",
            " 89%|████████▉ | 844/945 [00:41<00:04, 20.86it/s]\u001b[A\n",
            " 90%|████████▉ | 847/945 [00:41<00:04, 21.03it/s]\u001b[A\n",
            " 90%|████████▉ | 850/945 [00:42<00:06, 15.53it/s]\u001b[A\n",
            " 90%|█████████ | 853/945 [00:42<00:05, 16.95it/s]\u001b[A\n",
            " 90%|█████████ | 855/945 [00:42<00:05, 17.52it/s]\u001b[A\n",
            " 91%|█████████ | 857/945 [00:42<00:04, 18.03it/s]\u001b[A\n",
            " 91%|█████████ | 860/945 [00:42<00:04, 18.60it/s]\u001b[A\n",
            " 91%|█████████ | 862/945 [00:42<00:04, 18.75it/s]\u001b[A\n",
            " 92%|█████████▏| 865/945 [00:42<00:04, 19.55it/s]\u001b[A\n",
            " 92%|█████████▏| 868/945 [00:43<00:03, 20.14it/s]\u001b[A\n",
            " 92%|█████████▏| 871/945 [00:43<00:03, 20.22it/s]\u001b[A\n",
            " 92%|█████████▏| 874/945 [00:43<00:03, 20.59it/s]\u001b[A\n",
            " 93%|█████████▎| 877/945 [00:43<00:03, 20.86it/s]\u001b[A\n",
            " 93%|█████████▎| 880/945 [00:43<00:03, 21.00it/s]\u001b[A\n",
            " 93%|█████████▎| 883/945 [00:43<00:02, 21.02it/s]\u001b[A\n",
            " 94%|█████████▍| 886/945 [00:43<00:02, 20.99it/s]\u001b[A\n",
            " 94%|█████████▍| 889/945 [00:44<00:02, 21.09it/s]\u001b[A\n",
            " 94%|█████████▍| 892/945 [00:44<00:02, 21.15it/s]\u001b[A\n",
            " 95%|█████████▍| 895/945 [00:44<00:02, 21.17it/s]\u001b[A\n",
            " 95%|█████████▌| 898/945 [00:44<00:02, 21.25it/s]\u001b[A\n",
            " 95%|█████████▌| 901/945 [00:44<00:02, 21.03it/s]\u001b[A\n",
            " 96%|█████████▌| 904/945 [00:44<00:01, 20.99it/s]\u001b[A\n",
            " 96%|█████████▌| 907/945 [00:44<00:01, 20.77it/s]\u001b[A\n",
            " 96%|█████████▋| 910/945 [00:45<00:01, 20.70it/s]\u001b[A\n",
            " 97%|█████████▋| 913/945 [00:45<00:01, 20.69it/s]\u001b[A\n",
            " 97%|█████████▋| 916/945 [00:45<00:01, 20.56it/s]\u001b[A\n",
            " 97%|█████████▋| 919/945 [00:45<00:01, 20.91it/s]\u001b[A\n",
            " 98%|█████████▊| 922/945 [00:45<00:01, 21.06it/s]\u001b[A\n",
            " 98%|█████████▊| 925/945 [00:45<00:00, 20.95it/s]\u001b[A\n",
            " 98%|█████████▊| 928/945 [00:45<00:00, 21.12it/s]\u001b[A\n",
            " 99%|█████████▊| 931/945 [00:46<00:00, 20.67it/s]\u001b[A\n",
            " 99%|█████████▉| 934/945 [00:46<00:00, 20.73it/s]\u001b[A\n",
            " 99%|█████████▉| 937/945 [00:46<00:00, 20.62it/s]\u001b[A\n",
            " 99%|█████████▉| 940/945 [00:46<00:00, 20.45it/s]\u001b[A\n",
            "100%|██████████| 945/945 [00:46<00:00, 20.17it/s]\n"
          ]
        }
      ],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[:100])"
      ],
      "metadata": {
        "id": "vHHw8qub-dR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjcammuCxMKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48ed6f6-6b23-4757-fedd-30fa1f210347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.5 / client 1.5.8)\n",
            "100% 19.3M/19.3M [00:03<00:00, 5.75MB/s]\n",
            "Successfully submitted to 11785-HW1P2-S24"
          ]
        }
      ],
      "source": [
        "### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
        "!kaggle competitions submit -c 11785-hw1p2-s24 -f ./submission.csv -m \"Test Submission\"\n",
        "\n",
        "### However, its always safer to download the csv file and then upload to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDBXipbEObeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eccfba5e697c43609e3f2633e8128016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87797df5ff164cfa9cce755176a25d04",
              "IPY_MODEL_e8f45dfbd38b4666a8bcd0ff1f03bf36"
            ],
            "layout": "IPY_MODEL_295eb181c4fc4bbfb241c6b9141f8b6b"
          }
        },
        "87797df5ff164cfa9cce755176a25d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a982f191b49d4d9b9c85508bf73485e0",
            "placeholder": "​",
            "style": "IPY_MODEL_104e43c991944d3499eed0835ccd5d5c",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "e8f45dfbd38b4666a8bcd0ff1f03bf36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b6a898337c49db85a8e753f245f926",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b90a89d1f31941d18c05643b5ca222f7",
            "value": 1
          }
        },
        "295eb181c4fc4bbfb241c6b9141f8b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a982f191b49d4d9b9c85508bf73485e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104e43c991944d3499eed0835ccd5d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87b6a898337c49db85a8e753f245f926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90a89d1f31941d18c05643b5ca222f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41826af843da451bade66f42fb750dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ba8aacde30a400e8d69e6f1ef1f48ae",
              "IPY_MODEL_7f840088162243b29a9d006841c01394"
            ],
            "layout": "IPY_MODEL_9e402bfb24fd4d68ac5decb56c2c881b"
          }
        },
        "4ba8aacde30a400e8d69e6f1ef1f48ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43da8606c90e479d896307e8349979e7",
            "placeholder": "​",
            "style": "IPY_MODEL_b1f53fa7ae1a4e1e8d7751c7482901bb",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "7f840088162243b29a9d006841c01394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92f9062fdf5b4f6daeb22e28bed9d673",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4659073177ff4346b366661bd74340d8",
            "value": 1
          }
        },
        "9e402bfb24fd4d68ac5decb56c2c881b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43da8606c90e479d896307e8349979e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f53fa7ae1a4e1e8d7751c7482901bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92f9062fdf5b4f6daeb22e28bed9d673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4659073177ff4346b366661bd74340d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}